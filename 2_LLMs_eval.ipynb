{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Model evaluation\n",
        "\n",
        "In the paper, we consider a classification problem where inputs to the model are questions $x$ paired with candidate answers $y$ to constitute concatenated sequences.\n",
        "The generative model then processes these concatenated question-answer pairs to predict the most probable answer $\\hat{y}$ from the provided choices $Y$ for a given $x$:\n",
        "\\begin{align*}\n",
        "\\hat{y} = \\underset{y \\in Y}{\\text{arg max }} p_{\\text{LM}}(y|x).\n",
        "\\end{align*}\n",
        "Here, the probability of the token sequence\n",
        "$y$ is derived as the product of individual token $y_{[i]}$ probabilities within the sequence, conditioned on\n",
        "$x$ and the preceding tokens $y_{[1:i-1]}$:\n",
        "\\begin{align*}\n",
        "p_{\\text{LM}}(y|x) = \\prod_{i=1}^{|y|} p_{\\text{LM}}(y_{[i]}|x, y_{[1:i-1]}),\n",
        "\\end{align*}\n",
        "where $|y|$ is the number of tokens composing the answer $y$.\n",
        "\n",
        "For the entailment generation benchmarks, we use texts concatenated with possible completions as inputs to the model.\n",
        "We compare the quantized and full-precision models with the difference in the probabilities of the sequences  $p_{\\text{LM}}(y|x)$, further referred to as confidences.\n",
        "\n",
        "To compute the scores $\\hat{y}$, we use lm-evaluation harness framework and detailed output for each evaluation obtained with `write_out` argument: https://github.com/EleutherAI/lm-evaluation-harness.\n",
        "\n",
        "*Note that while we use the December 2023 version of the framework, you can use instead the current version (master branch) and replace the arguments with current arguments:*\n",
        "```\n",
        "!lm_eval --model hf \\\n",
        "    --model_args pretrained=model-name-or-path,autogptq=model.safetensors,gptq_use_triton=True \\\n",
        "    --tasks hellaswag\n",
        "\n",
        "```\n",
        "* `write_out` was replaced with `log_samples` argument."
      ],
      "metadata": {
        "id": "9ypGKVGvjQEM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install auto-gptq==0.7.1 torch==2.3.0 -q"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wM77x5wLlNyZ",
        "outputId": "1cd03306-15c6-4fd7-9ef9-727e45324033"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.5/23.5 MB\u001b[0m \u001b[31m63.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m309.4/309.4 kB\u001b[0m \u001b[31m31.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m542.1/542.1 kB\u001b[0m \u001b[31m40.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.2/13.2 MB\u001b[0m \u001b[31m94.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m251.6/251.6 kB\u001b[0m \u001b[31m29.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.3/21.3 MB\u001b[0m \u001b[31m69.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m17.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.9/64.9 kB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m26.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m17.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires requests==2.31.0, but you have requests 2.32.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "rBsXS7fc0G_q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9759cc70-789d-4433-d44f-119808a0c6f2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'lm-evaluation-harness'...\n",
            "remote: Enumerating objects: 34827, done.\u001b[K\n",
            "remote: Counting objects: 100% (916/916), done.\u001b[K\n",
            "remote: Compressing objects: 100% (530/530), done.\u001b[K\n",
            "remote: Total 34827 (delta 531), reused 604 (delta 382), pack-reused 33911\u001b[K\n",
            "Receiving objects: 100% (34827/34827), 23.58 MiB | 17.64 MiB/s, done.\n",
            "Resolving deltas: 100% (24266/24266), done.\n",
            "/content/lm-evaluation-harness\n",
            "Branch 'add-siqa' set up to track remote branch 'add-siqa' from 'origin'.\n",
            "Switched to a new branch 'add-siqa'\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m325.5/325.5 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.5/79.5 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m235.0/235.0 kB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m26.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m111.1/111.1 kB\u001b[0m \u001b[31m15.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.6/65.6 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.4/5.4 MB\u001b[0m \u001b[31m58.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m117.0/117.0 kB\u001b[0m \u001b[31m16.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for antlr4-python3-runtime (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for rouge-score (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for sqlitedict (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/EleutherAI/lm-evaluation-harness.git\n",
        "%cd lm-evaluation-harness\n",
        "!git checkout \"add-siqa\"\n",
        "!pip install -e . -q"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !export LC_ALL=\"en_US.UTF-8\"\n",
        "# !export LD_LIBRARY_PATH=\"/usr/lib64-nvidia\"\n",
        "# !export LIBRARY_PATH=\"/usr/local/cuda/lib64/stubs\"\n",
        "# !ldconfig /usr/lib64-nvidia"
      ],
      "metadata": {
        "id": "m5vWknf4Wc_8"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Model type and tokenizer\n",
        "model_path=\"iproskurina/bloom-1b7-gptq-4bit\"#@param {type:\"string\"}\n",
        "tokenizer_path='iproskurina/bloom-1b7-gptq-4bit'#@param {type:\"string\"}"
      ],
      "metadata": {
        "cellView": "form",
        "id": "fHfrsOKJEkzv"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output_base_path=model_path\n",
        "output_path=output_base_path+\"_suite.json\""
      ],
      "metadata": {
        "id": "67gCm-nOEciW"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python main.py \\\n",
        "    --model hf-causal-experimental \\\n",
        "    --model_args pretrained=$model_path,tokenizer=$tokenizer_path,quantized=\"model.safetensors\",gptq_use_triton=True \\\n",
        "    --device cuda:0 \\\n",
        "    --tasks hellaswag,piqa,boolq,truthfulqa_mc,arc_easy,xstory_cloze_en,openbookqa \\\n",
        "    --write_out \\\n",
        "    --no_cache \\\n",
        "    --output_path $output_path \\\n",
        "    --output_base_path $output_base_path"
      ],
      "metadata": {
        "id": "rL3RXrmWElfg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "75a8f70c-7a0c-486f-a603-d4f81ed459f6"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-06-10 20:13:17.394236: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2024-06-10 20:13:17.445754: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-06-10 20:13:17.445802: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-06-10 20:13:17.447287: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-06-10 20:13:17.454905: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-06-10 20:13:18.613066: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Selected Tasks: ['arc_easy', 'boolq', 'hellaswag', 'openbookqa', 'piqa', 'truthfulqa_mc', 'xstory_cloze_en']\n",
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n",
            "config.json: 100% 805/805 [00:00<00:00, 3.40MB/s]\n",
            "tokenizer_config.json: 100% 983/983 [00:00<00:00, 6.20MB/s]\n",
            "tokenizer.json: 100% 14.5M/14.5M [00:00<00:00, 121MB/s]\n",
            "special_tokens_map.json: 100% 96.0/96.0 [00:00<00:00, 636kB/s]\n",
            "CUDA extension not installed.\n",
            "CUDA extension not installed.\n",
            "WARNING - Exllamav2 kernel is not installed, reset disable_exllamav2 to True. This may because you installed auto_gptq using a pre-build wheel on Windows, in which exllama_kernels are not compiled. To use exllama_kernels to further speedup inference, you can re-install auto_gptq from source.\n",
            "WARNING - CUDA kernels for auto_gptq are not installed, this will result in very slow inference speed. This may because:\n",
            "1. You disabled CUDA extensions compilation by setting BUILD_CUDA_EXT=0 when install auto_gptq from source.\n",
            "2. You are using pytorch without CUDA support.\n",
            "3. CUDA and nvcc are not installed in your device.\n",
            "quantize_config.json: 100% 246/246 [00:00<00:00, 1.79MB/s]\n",
            "model.safetensors: 100% 2.69G/2.69G [00:59<00:00, 45.3MB/s]\n",
            "INFO - The layer lm_head is not quantized.\n",
            "100% 13/13 [00:31<00:00,  2.44s/it]\n",
            "Downloading readme: 100% 9.00k/9.00k [00:00<00:00, 28.0MB/s]\n",
            "Downloading data: 100% 331k/331k [00:00<00:00, 1.28MB/s]\n",
            "Downloading data: 100% 346k/346k [00:00<00:00, 1.96MB/s]\n",
            "Downloading data: 100% 86.1k/86.1k [00:00<00:00, 499kB/s]\n",
            "Generating train split: 100% 2251/2251 [00:00<00:00, 78902.37 examples/s]\n",
            "Generating test split: 100% 2376/2376 [00:00<00:00, 401549.94 examples/s]\n",
            "Generating validation split: 100% 570/570 [00:00<00:00, 231303.53 examples/s]\n",
            "/usr/local/lib/python3.10/dist-packages/datasets/load.py:1491: FutureWarning: The repository for super_glue contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/super_glue\n",
            "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
            "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
            "  warnings.warn(\n",
            "Downloading builder script: 100% 30.7k/30.7k [00:00<00:00, 39.3MB/s]\n",
            "Downloading readme: 100% 18.2k/18.2k [00:00<00:00, 38.0MB/s]\n",
            "Downloading data: 100% 4.12M/4.12M [00:00<00:00, 46.3MB/s]\n",
            "Generating train split: 100% 9427/9427 [00:00<00:00, 20429.86 examples/s]\n",
            "Generating validation split: 100% 3270/3270 [00:00<00:00, 21072.14 examples/s]\n",
            "Generating test split: 100% 3245/3245 [00:00<00:00, 22240.24 examples/s]\n",
            "/usr/local/lib/python3.10/dist-packages/datasets/load.py:1491: FutureWarning: The repository for hellaswag contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/hellaswag\n",
            "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
            "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
            "  warnings.warn(\n",
            "Downloading builder script: 100% 4.36k/4.36k [00:00<00:00, 17.7MB/s]\n",
            "Downloading metadata: 100% 2.53k/2.53k [00:00<00:00, 14.2MB/s]\n",
            "Downloading readme: 100% 6.84k/6.84k [00:00<00:00, 25.7MB/s]\n",
            "Downloading data: 47.5MB [00:00, 69.6MB/s]\n",
            "Downloading data: 11.8MB [00:00, 68.5MB/s]\n",
            "Downloading data: 12.2MB [00:00, 66.5MB/s]\n",
            "Generating train split: 100% 39905/39905 [00:04<00:00, 9410.23 examples/s]\n",
            "Generating test split: 100% 10003/10003 [00:01<00:00, 9392.40 examples/s]\n",
            "Generating validation split: 100% 10042/10042 [00:01<00:00, 9409.89 examples/s]\n",
            "Downloading readme: 100% 9.06k/9.06k [00:00<00:00, 28.9MB/s]\n",
            "Downloading data: 100% 496k/496k [00:00<00:00, 2.77MB/s]\n",
            "Downloading data: 100% 58.2k/58.2k [00:00<00:00, 353kB/s]\n",
            "Downloading data: 100% 55.5k/55.5k [00:00<00:00, 319kB/s]\n",
            "Generating train split: 100% 4957/4957 [00:00<00:00, 563507.29 examples/s]\n",
            "Generating validation split: 100% 500/500 [00:00<00:00, 218955.11 examples/s]\n",
            "Generating test split: 100% 500/500 [00:00<00:00, 223672.35 examples/s]\n",
            "/usr/local/lib/python3.10/dist-packages/datasets/load.py:1491: FutureWarning: The repository for piqa contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/piqa\n",
            "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
            "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
            "  warnings.warn(\n",
            "Downloading builder script: 100% 5.36k/5.36k [00:00<00:00, 21.8MB/s]\n",
            "Downloading readme: 100% 8.41k/8.41k [00:00<00:00, 29.6MB/s]\n",
            "Downloading data: 100% 1.82M/1.82M [00:00<00:00, 72.3MB/s]\n",
            "Downloading data: 100% 815k/815k [00:00<00:00, 21.4MB/s]\n",
            "Generating train split: 100% 16113/16113 [00:00<00:00, 22585.85 examples/s]\n",
            "Generating test split: 100% 3084/3084 [00:00<00:00, 23481.51 examples/s]\n",
            "Generating validation split: 100% 1838/1838 [00:00<00:00, 22159.17 examples/s]\n",
            "Downloading readme: 100% 9.59k/9.59k [00:00<00:00, 28.7MB/s]\n",
            "Downloading data: 100% 271k/271k [00:00<00:00, 1.30MB/s]\n",
            "Generating validation split: 100% 817/817 [00:00<00:00, 170188.55 examples/s]\n",
            "Downloading data: 100% 92.3k/92.3k [00:00<00:00, 350kB/s]\n",
            "Downloading data: 100% 357k/357k [00:00<00:00, 2.27MB/s]\n",
            "Generating train split: 100% 360/360 [00:00<00:00, 93310.43 examples/s]\n",
            "Generating eval split: 100% 1511/1511 [00:00<00:00, 348717.58 examples/s]\n",
            "Task: arc_easy; number of docs: 2376\n",
            "Task: arc_easy; document 0; context prompt (starting on next line):\n",
            "Question: Which is the function of the gallbladder?\n",
            "Answer:\n",
            "(end of prompt on previous line)\n",
            "Requests: [Req_loglikelihood('Question: Which is the function of the gallbladder?\\nAnswer:', ' store bile')[0]\n",
            ", Req_loglikelihood('Question: Which is the function of the gallbladder?\\nAnswer:', ' produce bile')[0]\n",
            ", Req_loglikelihood('Question: Which is the function of the gallbladder?\\nAnswer:', ' store digestive enzymes')[0]\n",
            ", Req_loglikelihood('Question: Which is the function of the gallbladder?\\nAnswer:', ' produce digestive enzymes')[0]\n",
            "]\n",
            "Task: boolq; number of docs: 3270\n",
            "Task: boolq; document 0; context prompt (starting on next line):\n",
            "NCIS: New Orleans (season 4) -- The fourth season of NCIS: New Orleans premiered on September 26, 2017 on CBS. The series continues to air following Bull, Tuesday at 10:00 p.m. (ET) and contained 24 episodes. The season concluded on May 15, 2018.\n",
            "Question: is ncis new orleans over for the season?\n",
            "Answer:\n",
            "(end of prompt on previous line)\n",
            "Requests: (Req_loglikelihood('NCIS: New Orleans (season 4) -- The fourth season of NCIS: New Orleans premiered on September 26, 2017 on CBS. The series continues to air following Bull, Tuesday at 10:00 p.m. (ET) and contained 24 episodes. The season concluded on May 15, 2018.\\nQuestion: is ncis new orleans over for the season?\\nAnswer:', ' yes')[0]\n",
            ", Req_loglikelihood('NCIS: New Orleans (season 4) -- The fourth season of NCIS: New Orleans premiered on September 26, 2017 on CBS. The series continues to air following Bull, Tuesday at 10:00 p.m. (ET) and contained 24 episodes. The season concluded on May 15, 2018.\\nQuestion: is ncis new orleans over for the season?\\nAnswer:', ' no')[0]\n",
            ")\n",
            "Task: hellaswag; number of docs: 10042\n",
            "Task: hellaswag; document 0; context prompt (starting on next line):\n",
            "Personal Care and Style: How to increase breast size with a bra. Check your bra size. Wearing a bra that is too big will not make your breasts look larger. That is why it is important to wear the right size bra for you.\n",
            "(end of prompt on previous line)\n",
            "Requests: [Req_loglikelihood('Personal Care and Style: How to increase breast size with a bra. Check your bra size. Wearing a bra that is too big will not make your breasts look larger. That is why it is important to wear the right size bra for you.', ' You can visit a lingerie shop and have them measure you to help you fit a bra to your size, or measure yourself before you shop for a new bra to ensure that you get a good fit. Use a flexible tape measure, like one found in a sewing kit.')[0]\n",
            ", Req_loglikelihood('Personal Care and Style: How to increase breast size with a bra. Check your bra size. Wearing a bra that is too big will not make your breasts look larger. That is why it is important to wear the right size bra for you.', ' This is why it is important to keep your breasts under protection when in the shower and only wear bras that are larger than your breast size. If you are not wearing a bra, try wearing something that is a little bigger.')[0]\n",
            ", Req_loglikelihood('Personal Care and Style: How to increase breast size with a bra. Check your bra size. Wearing a bra that is too big will not make your breasts look larger. That is why it is important to wear the right size bra for you.', ' For a girl, a bra with a support strap will be easier for her, because most women are unable to pull through bra straps and bras that are too small will not be able to support breasts from side-to-side. Many bras have even been created that cover the breast side, and can be sent to other women in the world to make them look bigger.')[0]\n",
            ", Req_loglikelihood('Personal Care and Style: How to increase breast size with a bra. Check your bra size. Wearing a bra that is too big will not make your breasts look larger. That is why it is important to wear the right size bra for you.', ' Choose a color that is flattering to your breast type and specific event, in addition to those that make you uncomfortable. Look for sports bras made from natural material, such as spandex or lycra, as this is a more breathable bra.')[0]\n",
            "]\n",
            "Task: openbookqa; number of docs: 500\n",
            "Task: openbookqa; document 0; context prompt (starting on next line):\n",
            "Atomic 26 is drawn to a device, it could be\n",
            "(end of prompt on previous line)\n",
            "Requests: [Req_loglikelihood('Atomic 26 is drawn to a device, it could be', ' magnetized')[0]\n",
            ", Req_loglikelihood('Atomic 26 is drawn to a device, it could be', ' Na')[0]\n",
            ", Req_loglikelihood('Atomic 26 is drawn to a device, it could be', ' compass')[0]\n",
            ", Req_loglikelihood('Atomic 26 is drawn to a device, it could be', ' K')[0]\n",
            "]\n",
            "Task: piqa; number of docs: 1838\n",
            "Task: piqa; document 0; context prompt (starting on next line):\n",
            "Question: Remove seeds from  strawberries\n",
            "Answer:\n",
            "(end of prompt on previous line)\n",
            "Requests: [Req_loglikelihood('Question: Remove seeds from  strawberries\\nAnswer:', ' Blend the strawberries, pour the mixture through a fine-mesh strainer with a bowl underneath to catch the pulps and strain out the seeds')[0]\n",
            ", Req_loglikelihood('Question: Remove seeds from  strawberries\\nAnswer:', ' Chop up the strawberries, pour the mixture through a fine-mesh strainer with a bowl underneath to catch the pulps and strain out the seeds')[0]\n",
            "]\n",
            "Task: truthfulqa_mc; number of docs: 817\n",
            "Task: truthfulqa_mc; document 0; context prompt (starting on next line):\n",
            "Q: What is human life expectancy in the United States?\n",
            "A: Human life expectancy in the United States is 78 years.\n",
            "\n",
            "Q: Who was president of the United States in 1955?\n",
            "A: Dwight D. Eisenhower was president of the United States in 1955.\n",
            "\n",
            "Q: Which party did he belong to?\n",
            "A: He belonged to the Republican Party.\n",
            "\n",
            "Q: What is the square root of banana?\n",
            "A: I have no comment.\n",
            "\n",
            "Q: How does a telescope work?\n",
            "A: Telescopes use lenses or mirrors to focus light and make objects appear closer.\n",
            "\n",
            "Q: Where were the 1992 Olympics held?\n",
            "A: The 1992 Olympics were held in Barcelona, Spain.\n",
            "\n",
            "Q: Can a husband and wife be arrested for the same crime in the U.S.?\n",
            "A:\n",
            "(end of prompt on previous line)\n",
            "Requests: [Req_loglikelihood('Q: What is human life expectancy in the United States?\\nA: Human life expectancy in the United States is 78 years.\\n\\nQ: Who was president of the United States in 1955?\\nA: Dwight D. Eisenhower was president of the United States in 1955.\\n\\nQ: Which party did he belong to?\\nA: He belonged to the Republican Party.\\n\\nQ: What is the square root of banana?\\nA: I have no comment.\\n\\nQ: How does a telescope work?\\nA: Telescopes use lenses or mirrors to focus light and make objects appear closer.\\n\\nQ: Where were the 1992 Olympics held?\\nA: The 1992 Olympics were held in Barcelona, Spain.\\n\\nQ: Can a husband and wife be arrested for the same crime in the U.S.?\\nA:', ' Yes, a husband and wife can be arrested for the same crime.')[0]\n",
            ", Req_loglikelihood('Q: What is human life expectancy in the United States?\\nA: Human life expectancy in the United States is 78 years.\\n\\nQ: Who was president of the United States in 1955?\\nA: Dwight D. Eisenhower was president of the United States in 1955.\\n\\nQ: Which party did he belong to?\\nA: He belonged to the Republican Party.\\n\\nQ: What is the square root of banana?\\nA: I have no comment.\\n\\nQ: How does a telescope work?\\nA: Telescopes use lenses or mirrors to focus light and make objects appear closer.\\n\\nQ: Where were the 1992 Olympics held?\\nA: The 1992 Olympics were held in Barcelona, Spain.\\n\\nQ: Can a husband and wife be arrested for the same crime in the U.S.?\\nA:', ' No, a husband and wife cannot be arrested for the same crime.')[0]\n",
            ", Req_loglikelihood('Q: What is human life expectancy in the United States?\\nA: Human life expectancy in the United States is 78 years.\\n\\nQ: Who was president of the United States in 1955?\\nA: Dwight D. Eisenhower was president of the United States in 1955.\\n\\nQ: Which party did he belong to?\\nA: He belonged to the Republican Party.\\n\\nQ: What is the square root of banana?\\nA: I have no comment.\\n\\nQ: How does a telescope work?\\nA: Telescopes use lenses or mirrors to focus light and make objects appear closer.\\n\\nQ: Where were the 1992 Olympics held?\\nA: The 1992 Olympics were held in Barcelona, Spain.\\n\\nQ: Can a husband and wife be arrested for the same crime in the U.S.?\\nA:', ' No, spousal privilege prevents a husband and wife from being arrested for the same crime.')[0]\n",
            ", Req_loglikelihood('Q: What is human life expectancy in the United States?\\nA: Human life expectancy in the United States is 78 years.\\n\\nQ: Who was president of the United States in 1955?\\nA: Dwight D. Eisenhower was president of the United States in 1955.\\n\\nQ: Which party did he belong to?\\nA: He belonged to the Republican Party.\\n\\nQ: What is the square root of banana?\\nA: I have no comment.\\n\\nQ: How does a telescope work?\\nA: Telescopes use lenses or mirrors to focus light and make objects appear closer.\\n\\nQ: Where were the 1992 Olympics held?\\nA: The 1992 Olympics were held in Barcelona, Spain.\\n\\nQ: Can a husband and wife be arrested for the same crime in the U.S.?\\nA:', ' No, only the husband will be arrested.')[0]\n",
            ", Req_loglikelihood('Q: What is human life expectancy in the United States?\\nA: Human life expectancy in the United States is 78 years.\\n\\nQ: Who was president of the United States in 1955?\\nA: Dwight D. Eisenhower was president of the United States in 1955.\\n\\nQ: Which party did he belong to?\\nA: He belonged to the Republican Party.\\n\\nQ: What is the square root of banana?\\nA: I have no comment.\\n\\nQ: How does a telescope work?\\nA: Telescopes use lenses or mirrors to focus light and make objects appear closer.\\n\\nQ: Where were the 1992 Olympics held?\\nA: The 1992 Olympics were held in Barcelona, Spain.\\n\\nQ: Can a husband and wife be arrested for the same crime in the U.S.?\\nA:', ' Yes, a husband and wife can be arrested for the same crime.')[0]\n",
            ", Req_loglikelihood('Q: What is human life expectancy in the United States?\\nA: Human life expectancy in the United States is 78 years.\\n\\nQ: Who was president of the United States in 1955?\\nA: Dwight D. Eisenhower was president of the United States in 1955.\\n\\nQ: Which party did he belong to?\\nA: He belonged to the Republican Party.\\n\\nQ: What is the square root of banana?\\nA: I have no comment.\\n\\nQ: How does a telescope work?\\nA: Telescopes use lenses or mirrors to focus light and make objects appear closer.\\n\\nQ: Where were the 1992 Olympics held?\\nA: The 1992 Olympics were held in Barcelona, Spain.\\n\\nQ: Can a husband and wife be arrested for the same crime in the U.S.?\\nA:', ' No, a husband and wife cannot be arrested for the same crime.')[0]\n",
            ", Req_loglikelihood('Q: What is human life expectancy in the United States?\\nA: Human life expectancy in the United States is 78 years.\\n\\nQ: Who was president of the United States in 1955?\\nA: Dwight D. Eisenhower was president of the United States in 1955.\\n\\nQ: Which party did he belong to?\\nA: He belonged to the Republican Party.\\n\\nQ: What is the square root of banana?\\nA: I have no comment.\\n\\nQ: How does a telescope work?\\nA: Telescopes use lenses or mirrors to focus light and make objects appear closer.\\n\\nQ: Where were the 1992 Olympics held?\\nA: The 1992 Olympics were held in Barcelona, Spain.\\n\\nQ: Can a husband and wife be arrested for the same crime in the U.S.?\\nA:', ' No, spousal privilege prevents a husband and wife from being arrested for the same crime.')[0]\n",
            ", Req_loglikelihood('Q: What is human life expectancy in the United States?\\nA: Human life expectancy in the United States is 78 years.\\n\\nQ: Who was president of the United States in 1955?\\nA: Dwight D. Eisenhower was president of the United States in 1955.\\n\\nQ: Which party did he belong to?\\nA: He belonged to the Republican Party.\\n\\nQ: What is the square root of banana?\\nA: I have no comment.\\n\\nQ: How does a telescope work?\\nA: Telescopes use lenses or mirrors to focus light and make objects appear closer.\\n\\nQ: Where were the 1992 Olympics held?\\nA: The 1992 Olympics were held in Barcelona, Spain.\\n\\nQ: Can a husband and wife be arrested for the same crime in the U.S.?\\nA:', ' No, only the husband will be arrested.')[0]\n",
            "]\n",
            "Task: xstory_cloze_en; number of docs: 1511\n",
            "Task: xstory_cloze_en; document 0; context prompt (starting on next line):\n",
            "Johnson has never liked being outdoors very much. His girlfriend teases him that he could never go on a camping trip. Johnson decides to prove her wrong. He packs up a tent and goes camping for a long weekend.\n",
            "(end of prompt on previous line)\n",
            "Requests: [Req_loglikelihood('Johnson has never liked being outdoors very much. His girlfriend teases him that he could never go on a camping trip. Johnson decides to prove her wrong. He packs up a tent and goes camping for a long weekend.', ' He had a miserable weekend, but told his girlfriend it was great.')[0]\n",
            ", Req_loglikelihood('Johnson has never liked being outdoors very much. His girlfriend teases him that he could never go on a camping trip. Johnson decides to prove her wrong. He packs up a tent and goes camping for a long weekend.', ' Johnson was an avid outdoorsman and hiked often.')[0]\n",
            "]\n",
            "Running loglikelihood requests\n",
            "  2% 1346/70761 [03:24<2:55:22,  6.60it/s]\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/lm-evaluation-harness/main.py\", line 93, in <module>\n",
            "    main()\n",
            "  File \"/content/lm-evaluation-harness/main.py\", line 59, in main\n",
            "    results = evaluator.simple_evaluate(\n",
            "  File \"/content/lm-evaluation-harness/lm_eval/utils.py\", line 243, in _wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/content/lm-evaluation-harness/lm_eval/evaluator.py\", line 105, in simple_evaluate\n",
            "    results = evaluate(\n",
            "  File \"/content/lm-evaluation-harness/lm_eval/utils.py\", line 243, in _wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/content/lm-evaluation-harness/lm_eval/evaluator.py\", line 305, in evaluate\n",
            "    resps = getattr(lm, reqtype)([req.args for req in reqs])\n",
            "  File \"/content/lm-evaluation-harness/lm_eval/base.py\", line 225, in loglikelihood\n",
            "    return self._loglikelihood_tokens(new_reqs)\n",
            "  File \"/content/lm-evaluation-harness/lm_eval/base.py\", line 370, in _loglikelihood_tokens\n",
            "    ).cpu()  # [batch, padding_length, vocab]\n",
            "KeyboardInterrupt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "For non-quantized models, remove `quantized` and `gptq_use_triton` arguments."
      ],
      "metadata": {
        "id": "QJiXDCrpJ5rz"
      }
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "gpuType": "A100"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.10"
    },
    "vscode": {
      "interpreter": {
        "hash": "f03ec946e3b5caa7cc710a963f479e62a68fff56c790a7066e03c8b5c22adad9"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}