{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Requirements"
      ],
      "metadata": {
        "id": "IB2LcIzZDvF-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc --version"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4dFKsPaea30b",
        "outputId": "58b8bf4d-56b3-4877-a93d-4a8a9c676ba1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nvcc: NVIDIA (R) Cuda compiler driver\n",
            "Copyright (c) 2005-2023 NVIDIA Corporation\n",
            "Built on Tue_Aug_15_22:02:13_PDT_2023\n",
            "Cuda compilation tools, release 12.2, V12.2.140\n",
            "Build cuda_12.2.r12.2/compiler.33191640_0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Requirements**\n",
        "\n",
        "Requires: Transformers 4.33.0 or later, Optimum 1.12.0 or later, and AutoGPTQ 0.4.2 or later.\n",
        "\n",
        "```shell\n",
        "pip3 install --upgrade transformers optimum\n",
        "# If using PyTorch 2.1 + CUDA 12.x:\n",
        "pip3 install --upgrade auto-gptq\n",
        "# or, if using PyTorch 2.1 + CUDA 11.x:\n",
        "pip3 install --upgrade auto-gptq --extra-index-url https://huggingface.github.io/autogptq-index/whl/cu118/\n",
        "```\n",
        "\n",
        "If you are using PyTorch 2.0, you will need to install AutoGPTQ from source. Likewise if you have problems with the pre-built wheels, you should try building from source:\n",
        "\n",
        "```shell\n",
        "pip3 uninstall -y auto-gptq\n",
        "git clone https://github.com/PanQiWei/AutoGPTQ\n",
        "cd AutoGPTQ\n",
        "git checkout v0.5.1\n",
        "pip3 install .\n",
        "```\n"
      ],
      "metadata": {
        "id": "Jz8qvgWq9Bi6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade transformers optimum\n",
        "# If using PyTorch 2.1 + CUDA 12.x:\n",
        "!pip install --upgrade auto-gptq\n",
        "# or, if using PyTorch 2.1 + CUDA 11.x:\n",
        "# !pip install --upgrade auto-gptq --extra-index-url https://huggingface.github.io/autogptq-index/whl/cu118/"
      ],
      "metadata": {
        "id": "bB0eBbo0bDea"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Check last available versions of the Auto-GPTQ here: https://github.com/AutoGPTQ/AutoGPTQ/blob/main/docs/INSTALLATION.md"
      ],
      "metadata": {
        "id": "WRu9bhYphdD4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Any\n",
        "import random\n",
        "import torch\n",
        "from datasets import load_dataset\n",
        "from transformers import AutoTokenizer, TextGenerationPipeline\n",
        "from auto_gptq import AutoGPTQForCausalLM, BaseQuantizeConfig\n",
        "import os\n",
        "def get_c4(tokenizer: Any, seqlen: int, nsamples: int, split: str = \"train\"):\n",
        "    if split == \"train\":\n",
        "        data = load_dataset(\"allenai/c4\", split=\"train\", data_files={\"train\": \"en/c4-train.00000-of-01024.json.gz\"})\n",
        "    elif split == \"validation\":\n",
        "        data = load_dataset(\n",
        "            \"allenai/c4\",\n",
        "            split=\"validation\",\n",
        "            data_files={\"validation\": \"en/c4-validation.00000-of-00008.json.gz\"},\n",
        "        )\n",
        "    dataset = []\n",
        "    for _ in range(nsamples):\n",
        "        while True:\n",
        "\n",
        "            i = random.randint(0, len(data) - 1)\n",
        "            enc = tokenizer(data[i][\"text\"], return_tensors=\"pt\")\n",
        "            if enc.input_ids.shape[1] >= seqlen:\n",
        "                break\n",
        "        if enc.input_ids.shape[1] - seqlen - 1 >0:\n",
        "            i = random.randint(0, enc.input_ids.shape[1] - seqlen - 1)\n",
        "            j = i + seqlen\n",
        "            inp = enc.input_ids[:, i:j]\n",
        "            attention_mask = torch.ones_like(inp)\n",
        "            dataset.append({\"input_ids\": inp, \"attention_mask\": attention_mask})\n",
        "    return dataset"
      ],
      "metadata": {
        "id": "RiAlcBRsCZwH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Choose a model for quantization\n",
        "pretrained_model_dir = \"facebook/opt-125m\" #@param str\n",
        "!echo proceed with model: {pretrained_model_dir}"
      ],
      "metadata": {
        "id": "eacWVC4k-u9s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Enter the desired bit precision (n-bit) for quantization (e.g., 2,3,4,8):\n",
        "n_bits = 4 #@param int"
      ],
      "metadata": {
        "id": "emC96azL_N4d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Quantization"
      ],
      "metadata": {
        "id": "9MXmQ8KfDAGG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_name=pretrained_model_dir.split(\"/\")[-1]\n",
        "n_bits=str(n_bits)\n",
        "quantized_model_dir = f\"{model_name}-{n_bits}bit\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(pretrained_model_dir, use_fast=True)\n",
        "examples=get_c4(tokenizer=tokenizer, seqlen=2048, nsamples=128, split=\"train\")\n",
        "\n",
        "quantize_config = BaseQuantizeConfig(\n",
        "    bits=n_bits,\n",
        "    group_size=128,\n",
        "    desc_act=False,\n",
        ")\n",
        "\n",
        "# load un-quantized model, by default, the model will always be loaded into CPU memory\n",
        "model = AutoGPTQForCausalLM.from_pretrained(pretrained_model_dir, quantize_config)\n",
        "\n",
        "# quantize model, the examples should be list of dict whose keys can only be \"input_ids\" and \"attention_mask\"\n",
        "model.quantize(examples)\n",
        "model.save_quantized(quantized_model_dir, use_safetensors=True)\n",
        "\n",
        "# **Note**: By default, the format of the model file base name saved using Auto-GPTQ is: gptq_model-{bits}bit-{group_size}g.\n",
        "# To support further loading with the automatic transformers class AutoForCausalLM, rename the file as below to model.safetensors.\n",
        "matching_file_weights=None\n",
        "for filename in os.listdir(quantized_model_dir):\n",
        "    if filename.endswith('.safetensors') and filename != 'model.safetensors':\n",
        "        matching_file_weights.append(filename)\n",
        "if matching_file_weights:\n",
        "    new_model_file = f'{quantized_model_dir}/model.safetensors'\n",
        "    os.rename(matching_file_weights, new_model_file)\n",
        "\n",
        "# Voil√†, now the model can be used for inference\n",
        "# load quantized model to the first GPU\n",
        "model = AutoGPTQForCausalLM.from_quantized(quantized_model_dir, device=\"cuda:0\")\n",
        "\n",
        "# inference with model.generate\n",
        "print(tokenizer.decode(model.generate(**tokenizer(\"auto_gptq is\", return_tensors=\"pt\").to(model.device))[0]))\n",
        "\n",
        "# or you can also use pipeline\n",
        "pipeline = TextGenerationPipeline(model=model, tokenizer=tokenizer)\n",
        "print(pipeline(\"auto-gptq is\")[0][\"generated_text\"])"
      ],
      "metadata": {
        "id": "DucYBSidbI0X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Push Quantized Model to Hugging Face Hub\n",
        "\n",
        "To use `use_auth_token=True`, log in first via `huggingface-cli login`, or pass an explicit token with: `use_auth_token=\"hf_xxxxxxx\"`.\n",
        "\n",
        "**Uncomment the following three lines to enable this feature:**\n",
        "\n",
        "```python\n",
        "repo_id = f\"YourUserName/{quantized_model_dir}\"\n",
        "commit_message = f\"AutoGPTQ model for {pretrained_model_dir}: {quantize_config.bits} bits, gr{quantize_config.group_size}, desc_act={quantize_config.desc_act}\"\n",
        "```\n",
        "**Note**: By default, the format of the model file base name saved using Auto-GPTQ is: `gptq_model-{bits}bit-{group_size}g`. To support further loading with the automatic class `AutoForCausalLM`, change it to `model.safetensors`, as suggested above.\n",
        "\n",
        "```\n",
        "model.push_to_hub(repo_id, commit_message=commit_message, use_auth_token=True)\n",
        "tokenizer.push_to_hub(repo_id, commit_message=commit_message, use_auth_token=True)\n",
        "```"
      ],
      "metadata": {
        "id": "mTbbGl5VCNNH"
      }
    }
  ]
}