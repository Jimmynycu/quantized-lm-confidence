{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ypGKVGvjQEM"
      },
      "source": [
        "# Model evaluation\n",
        "\n",
        "In the paper, we consider a classification problem where inputs to the model are questions $x$ paired with candidate answers $y$ to constitute concatenated sequences.\n",
        "The generative model then processes these concatenated question-answer pairs to predict the most probable answer $\\hat{y}$ from the provided choices $Y$ for a given $x$:\n",
        "\\begin{align*}\n",
        "\\hat{y} = \\underset{y \\in Y}{\\text{arg max }} p_{\\text{LM}}(y|x).\n",
        "\\end{align*}\n",
        "Here, the probability of the token sequence\n",
        "$y$ is derived as the product of individual token $y_{[i]}$ probabilities within the sequence, conditioned on\n",
        "$x$ and the preceding tokens $y_{[1:i-1]}$:\n",
        "\\begin{align*}\n",
        "p_{\\text{LM}}(y|x) = \\prod_{i=1}^{|y|} p_{\\text{LM}}(y_{[i]}|x, y_{[1:i-1]}),\n",
        "\\end{align*}\n",
        "where $|y|$ is the number of tokens composing the answer $y$.\n",
        "\n",
        "For the entailment generation benchmarks, we use texts concatenated with possible completions as inputs to the model.\n",
        "We compare the quantized and full-precision models with the difference in the probabilities of the sequences  $p_{\\text{LM}}(y|x)$, further referred to as confidences.\n",
        "\n",
        "To compute the scores $\\hat{y}$, we use lm-evaluation harness framework and detailed output for each evaluation obtained with `write_out` argument: https://github.com/EleutherAI/lm-evaluation-harness.\n",
        "\n",
        "*Note that while we use the December 2023 version of the framework, you can use instead the current version (master branch) and replace the arguments with current arguments:*\n",
        "```\n",
        "!lm_eval --model hf \\\n",
        "    --model_args pretrained=model-name-or-path,autogptq=model.safetensors,gptq_use_triton=True \\\n",
        "    --tasks hellaswag\n",
        "\n",
        "```\n",
        "* `write_out` was replaced with `log_samples` argument."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wM77x5wLlNyZ",
        "outputId": "1cd03306-15c6-4fd7-9ef9-727e45324033"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.5/23.5 MB\u001b[0m \u001b[31m63.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m309.4/309.4 kB\u001b[0m \u001b[31m31.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m542.1/542.1 kB\u001b[0m \u001b[31m40.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.2/13.2 MB\u001b[0m \u001b[31m94.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m251.6/251.6 kB\u001b[0m \u001b[31m29.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.3/21.3 MB\u001b[0m \u001b[31m69.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m17.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.9/64.9 kB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m26.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m17.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires requests==2.31.0, but you have requests 2.32.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install auto-gptq==0.7.1 torch==2.3.0 -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rBsXS7fc0G_q",
        "outputId": "9759cc70-789d-4433-d44f-119808a0c6f2"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/root/.local/lib/python3.10/site-packages/IPython/core/magics/osm.py:417: UserWarning: This is now an optional IPython functionality, setting dhist requires you to install the `pickleshare` library.\n",
            "  self.shell.db['dhist'] = compress_dhist(dhist)[-100:]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/root/quantized-lm-confidence/notebooks/lm-evaluation-harness\n",
            "Already on 'add-siqa'\n",
            "Your branch is up to date with 'origin/add-siqa'.\n",
            "\u001b[33mDEPRECATION: distro-info 0.23ubuntu1 has a non-standard version number. pip 23.3 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of distro-info or contact the author to suggest that they release a version with a conforming version number. Discussion can be found at https://github.com/pypa/pip/issues/12063\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: There was an error checking the latest version of pip.\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "# !git clone https://github.com/EleutherAI/lm-evaluation-harness.git\n",
        "%cd lm-evaluation-harness\n",
        "!git checkout \"add-siqa\"\n",
        "!pip install -e . -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "m5vWknf4Wc_8"
      },
      "outputs": [],
      "source": [
        "# !export LC_ALL=\"en_US.UTF-8\"\n",
        "# !export LD_LIBRARY_PATH=\"/usr/lib64-nvidia\"\n",
        "# !export LIBRARY_PATH=\"/usr/local/cuda/lib64/stubs\"\n",
        "# !ldconfig /usr/lib64-nvidia"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "cellView": "form",
        "id": "fHfrsOKJEkzv"
      },
      "outputs": [],
      "source": [
        "#@title Model type and tokenizer\n",
        "model_path=\"bigscience/bloom-1b7\"#@param {type:\"string\"}\n",
        "tokenizer_path='bigscience/bloom-1b7'#@param {type:\"string\"}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "67gCm-nOEciW"
      },
      "outputs": [],
      "source": [
        "output_base_path=model_path\n",
        "output_path=output_base_path+\"_suite.json\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rL3RXrmWElfg",
        "outputId": "75a8f70c-7a0c-486f-a603-d4f81ed459f6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Selected Tasks: ['hellaswag']\n",
            "Task: hellaswag; number of docs: 10042\n",
            "Task: hellaswag; document 0; context prompt (starting on next line):\n",
            "Personal Care and Style: How to increase breast size with a bra. Check your bra size. Wearing a bra that is too big will not make your breasts look larger. That is why it is important to wear the right size bra for you.\n",
            "(end of prompt on previous line)\n",
            "Requests: [Req_loglikelihood('Personal Care and Style: How to increase breast size with a bra. Check your bra size. Wearing a bra that is too big will not make your breasts look larger. That is why it is important to wear the right size bra for you.', ' You can visit a lingerie shop and have them measure you to help you fit a bra to your size, or measure yourself before you shop for a new bra to ensure that you get a good fit. Use a flexible tape measure, like one found in a sewing kit.')[0]\n",
            ", Req_loglikelihood('Personal Care and Style: How to increase breast size with a bra. Check your bra size. Wearing a bra that is too big will not make your breasts look larger. That is why it is important to wear the right size bra for you.', ' This is why it is important to keep your breasts under protection when in the shower and only wear bras that are larger than your breast size. If you are not wearing a bra, try wearing something that is a little bigger.')[0]\n",
            ", Req_loglikelihood('Personal Care and Style: How to increase breast size with a bra. Check your bra size. Wearing a bra that is too big will not make your breasts look larger. That is why it is important to wear the right size bra for you.', ' For a girl, a bra with a support strap will be easier for her, because most women are unable to pull through bra straps and bras that are too small will not be able to support breasts from side-to-side. Many bras have even been created that cover the breast side, and can be sent to other women in the world to make them look bigger.')[0]\n",
            ", Req_loglikelihood('Personal Care and Style: How to increase breast size with a bra. Check your bra size. Wearing a bra that is too big will not make your breasts look larger. That is why it is important to wear the right size bra for you.', ' Choose a color that is flattering to your breast type and specific event, in addition to those that make you uncomfortable. Look for sports bras made from natural material, such as spandex or lycra, as this is a more breathable bra.')[0]\n",
            "]\n",
            "Running loglikelihood requests\n",
            "100%|███████████████████████████████████| 40145/40145 [1:07:26<00:00,  9.92it/s]\n",
            "{\n",
            "  \"results\": {\n",
            "    \"hellaswag\": {\n",
            "      \"acc\": 0.3754232224656443,\n",
            "      \"acc_stderr\": 0.004832423630593185,\n",
            "      \"acc_norm\": 0.4797849034056961,\n",
            "      \"acc_norm_stderr\": 0.0049857015938980015\n",
            "    }\n",
            "  },\n",
            "  \"versions\": {\n",
            "    \"hellaswag\": 0\n",
            "  },\n",
            "  \"config\": {\n",
            "    \"model\": \"hf-causal-experimental\",\n",
            "    \"model_args\": \"pretrained=bigscience/bloom-1b7,tokenizer=bigscience/bloom-1b7\",\n",
            "    \"num_fewshot\": 0,\n",
            "    \"batch_size\": null,\n",
            "    \"batch_sizes\": [],\n",
            "    \"device\": \"cuda:0\",\n",
            "    \"no_cache\": true,\n",
            "    \"limit\": null,\n",
            "    \"bootstrap_iters\": 100000,\n",
            "    \"description_dict\": {}\n",
            "  }\n",
            "}\n",
            "hf-causal-experimental (pretrained=bigscience/bloom-1b7,tokenizer=bigscience/bloom-1b7), limit: None, provide_description: False, num_fewshot: 0, batch_size: None\n",
            "|  Task   |Version| Metric |Value |   |Stderr|\n",
            "|---------|------:|--------|-----:|---|-----:|\n",
            "|hellaswag|      0|acc     |0.3754|±  |0.0048|\n",
            "|         |       |acc_norm|0.4798|±  |0.0050|\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!python main.py \\\n",
        "    --model hf-causal-experimental \\\n",
        "    --model_args pretrained=$model_path,tokenizer=$tokenizer_path \\\n",
        "    --device cuda:0 \\\n",
        "    --tasks hellaswag \\\n",
        "    --write_out \\\n",
        "    --no_cache \\\n",
        "    --output_path $output_path \\\n",
        "    --output_base_path $output_base_path"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QJiXDCrpJ5rz"
      },
      "source": [
        "For non-quantized models, remove `quantized` and `gptq_use_triton` arguments."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "vscode": {
      "interpreter": {
        "hash": "f03ec946e3b5caa7cc710a963f479e62a68fff56c790a7066e03c8b5c22adad9"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
