{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ypGKVGvjQEM"
      },
      "source": [
        "# Model evaluation\n",
        "\n",
        "In the paper, we consider a classification problem where inputs to the model are questions $x$ paired with candidate answers $y$ to constitute concatenated sequences.\n",
        "The generative model then processes these concatenated question-answer pairs to predict the most probable answer $\\hat{y}$ from the provided choices $Y$ for a given $x$:\n",
        "\\begin{align*}\n",
        "\\hat{y} = \\underset{y \\in Y}{\\text{arg max }} p_{\\text{LM}}(y|x).\n",
        "\\end{align*}\n",
        "Here, the probability of the token sequence\n",
        "$y$ is derived as the product of individual token $y_{[i]}$ probabilities within the sequence, conditioned on\n",
        "$x$ and the preceding tokens $y_{[1:i-1]}$:\n",
        "\\begin{align*}\n",
        "p_{\\text{LM}}(y|x) = \\prod_{i=1}^{|y|} p_{\\text{LM}}(y_{[i]}|x, y_{[1:i-1]}),\n",
        "\\end{align*}\n",
        "where $|y|$ is the number of tokens composing the answer $y$.\n",
        "\n",
        "For the entailment generation benchmarks, we use texts concatenated with possible completions as inputs to the model.\n",
        "We compare the quantized and full-precision models with the difference in the probabilities of the sequences  $p_{\\text{LM}}(y|x)$, further referred to as confidences.\n",
        "\n",
        "To compute the scores $\\hat{y}$, we use lm-evaluation harness framework and detailed output for each evaluation obtained with `write_out` argument: https://github.com/EleutherAI/lm-evaluation-harness.\n",
        "\n",
        "*Note that while we use the December 2023 version of the framework, you can use instead the current version (master branch) and replace the arguments with current arguments:*\n",
        "```\n",
        "!lm_eval --model hf \\\n",
        "    --model_args pretrained=model-name-or-path,autogptq=model.safetensors,gptq_use_triton=True \\\n",
        "    --tasks hellaswag\n",
        "\n",
        "```\n",
        "* `write_out` was replaced with `log_samples` argument."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wM77x5wLlNyZ",
        "outputId": "1cd03306-15c6-4fd7-9ef9-727e45324033"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[33mDEPRECATION: distro-info 0.23ubuntu1 has a non-standard version number. pip 23.3 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of distro-info or contact the author to suggest that they release a version with a conforming version number. Discussion can be found at https://github.com/pypa/pip/issues/12063\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: There was an error checking the latest version of pip.\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install auto-gptq==0.7.1 torch==2.3.0 -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rBsXS7fc0G_q",
        "outputId": "9759cc70-789d-4433-d44f-119808a0c6f2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Errno 2] No such file or directory: 'lm-evaluation-harness'\n",
            "/root/quantized-lm-confidence/notebooks/lm-evaluation-harness\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/root/.local/lib/python3.10/site-packages/IPython/core/magics/osm.py:393: UserWarning: This is now an optional IPython functionality, using bookmarks requires you to install the `pickleshare` library.\n",
            "  bkms = self.shell.db.get('bookmarks', {})\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Already on 'add-siqa'\n",
            "Your branch is up to date with 'origin/add-siqa'.\n",
            "\u001b[33mDEPRECATION: distro-info 0.23ubuntu1 has a non-standard version number. pip 23.3 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of distro-info or contact the author to suggest that they release a version with a conforming version number. Discussion can be found at https://github.com/pypa/pip/issues/12063\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: There was an error checking the latest version of pip.\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "# !git clone https://github.com/EleutherAI/lm-evaluation-harness.git\n",
        "%cd lm-evaluation-harness\n",
        "!git checkout \"add-siqa\"\n",
        "!pip install -e . -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "m5vWknf4Wc_8"
      },
      "outputs": [],
      "source": [
        "# !export LC_ALL=\"en_US.UTF-8\"\n",
        "# !export LD_LIBRARY_PATH=\"/usr/lib64-nvidia\"\n",
        "# !export LIBRARY_PATH=\"/usr/local/cuda/lib64/stubs\"\n",
        "# !ldconfig /usr/lib64-nvidia"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "cellView": "form",
        "id": "fHfrsOKJEkzv"
      },
      "outputs": [],
      "source": [
        "#@title Model type and tokenizer\n",
        "model_path=\"Jimmy1229/Llama-3.2-1B-4bit\"#@param {type:\"string\"}\n",
        "tokenizer_path='Jimmy1229/Llama-3.2-1B-4bit'#@param {type:\"string\"}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "67gCm-nOEciW"
      },
      "outputs": [],
      "source": [
        "output_base_path=model_path\n",
        "output_path=output_base_path+\"_suite.json\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rL3RXrmWElfg",
        "outputId": "75a8f70c-7a0c-486f-a603-d4f81ed459f6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Selected Tasks: ['hellaswag']\n",
            "Traceback (most recent call last):\n",
            "  File \"/root/quantized-lm-confidence/notebooks/lm-evaluation-harness/main.py\", line 93, in <module>\n",
            "    main()\n",
            "  File \"/root/quantized-lm-confidence/notebooks/lm-evaluation-harness/main.py\", line 59, in main\n",
            "    results = evaluator.simple_evaluate(\n",
            "  File \"/root/quantized-lm-confidence/notebooks/lm-evaluation-harness/lm_eval/utils.py\", line 243, in _wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/root/quantized-lm-confidence/notebooks/lm-evaluation-harness/lm_eval/evaluator.py\", line 76, in simple_evaluate\n",
            "    lm = lm_eval.models.get_model(model).create_from_arg_string(\n",
            "  File \"/root/quantized-lm-confidence/notebooks/lm-evaluation-harness/lm_eval/base.py\", line 115, in create_from_arg_string\n",
            "    return cls(**args, **args2)\n",
            "  File \"/root/quantized-lm-confidence/notebooks/lm-evaluation-harness/lm_eval/models/huggingface.py\", line 201, in __init__\n",
            "    self.tokenizer = self._create_auto_tokenizer(\n",
            "  File \"/root/quantized-lm-confidence/notebooks/lm-evaluation-harness/lm_eval/models/huggingface.py\", line 508, in _create_auto_tokenizer\n",
            "    tokenizer = super()._create_auto_tokenizer(\n",
            "  File \"/root/quantized-lm-confidence/notebooks/lm-evaluation-harness/lm_eval/models/huggingface.py\", line 339, in _create_auto_tokenizer\n",
            "    tokenizer = self.AUTO_TOKENIZER_CLASS.from_pretrained(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/transformers/models/auto/tokenization_auto.py\", line 953, in from_pretrained\n",
            "    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py\", line 2020, in from_pretrained\n",
            "    raise EnvironmentError(\n",
            "OSError: Can't load tokenizer for 'meta-llama/Llama-3.2-1B'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'meta-llama/Llama-3.2-1B' is the correct path to a directory containing all relevant files for a LlamaTokenizerFast tokenizer.\n"
          ]
        }
      ],
      "source": [
        "!python main.py \\\n",
        "    --model hf-causal-experimental \\\n",
        "    --model_args pretrained=$model_path,tokenizer=$tokenizer_path,quantized=\"model.safetensors\",gptq_use_triton=True,trust_remote_code=True\\\n",
        "    --device cuda:0 \\\n",
        "    --tasks hellaswag \\\n",
        "    --write_out \\\n",
        "    --no_cache \\\n",
        "    --output_path $output_path \\\n",
        "    --output_base_path $output_base_path"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QJiXDCrpJ5rz"
      },
      "source": [
        "For non-quantized models, remove `quantized` and `gptq_use_triton` arguments."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
            "Token is valid (permission: fineGrained).\n",
            "The token `3B` has been saved to /root/.cache/huggingface/stored_tokens\n",
            "Your token has been saved to /root/.cache/huggingface/token\n",
            "Login successful.\n",
            "The current active token is: `3B`\n",
            "Selected Tasks: ['piqa', 'truthfulqa_mc']\n",
            "Traceback (most recent call last):\n",
            "  File \"/root/quantized-lm-confidence/notebooks/lm-evaluation-harness/main.py\", line 93, in <module>\n",
            "    main()\n",
            "  File \"/root/quantized-lm-confidence/notebooks/lm-evaluation-harness/main.py\", line 59, in main\n",
            "    results = evaluator.simple_evaluate(\n",
            "  File \"/root/quantized-lm-confidence/notebooks/lm-evaluation-harness/lm_eval/utils.py\", line 243, in _wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/root/quantized-lm-confidence/notebooks/lm-evaluation-harness/lm_eval/evaluator.py\", line 76, in simple_evaluate\n",
            "    lm = lm_eval.models.get_model(model).create_from_arg_string(\n",
            "  File \"/root/quantized-lm-confidence/notebooks/lm-evaluation-harness/lm_eval/base.py\", line 115, in create_from_arg_string\n",
            "    return cls(**args, **args2)\n",
            "  File \"/root/quantized-lm-confidence/notebooks/lm-evaluation-harness/lm_eval/models/huggingface.py\", line 201, in __init__\n",
            "    self.tokenizer = self._create_auto_tokenizer(\n",
            "  File \"/root/quantized-lm-confidence/notebooks/lm-evaluation-harness/lm_eval/models/huggingface.py\", line 508, in _create_auto_tokenizer\n",
            "    tokenizer = super()._create_auto_tokenizer(\n",
            "  File \"/root/quantized-lm-confidence/notebooks/lm-evaluation-harness/lm_eval/models/huggingface.py\", line 339, in _create_auto_tokenizer\n",
            "    tokenizer = self.AUTO_TOKENIZER_CLASS.from_pretrained(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/transformers/models/auto/tokenization_auto.py\", line 953, in from_pretrained\n",
            "    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py\", line 2020, in from_pretrained\n",
            "    raise EnvironmentError(\n",
            "OSError: Can't load tokenizer for 'meta-llama/Llama-3.2-1B'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'meta-llama/Llama-3.2-1B' is the correct path to a directory containing all relevant files for a LlamaTokenizerFast tokenizer.\n"
          ]
        }
      ],
      "source": [
        "!huggingface-cli login --token \n",
        "#@title Model type and tokenizer\n",
        "model_path=\"meta-llama/Llama-3.2-1B\"#@param {type:\"string\"}\n",
        "tokenizer_path='meta-llama/Llama-3.2-1B'#@param {type:\"string\"}\n",
        "output_base_path=model_path\n",
        "output_path=output_base_path+\"_suite.json\"\n",
        "!python main.py \\\n",
        "    --model hf-causal-experimental \\\n",
        "    --model_args pretrained=$model_path,tokenizer=$tokenizer_path\\\n",
        "    --device cuda:0 \\\n",
        "    --tasks piqa,truthfulqa_mc \\\n",
        "    --write_out \\\n",
        "    --no_cache \\\n",
        "    --output_path $output_path \\\n",
        "    --output_base_path $output_base_path"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "vscode": {
      "interpreter": {
        "hash": "f03ec946e3b5caa7cc710a963f479e62a68fff56c790a7066e03c8b5c22adad9"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
