{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0mT2KWpb6wWo"
      },
      "source": [
        "# Computing Calibration Metrics\n",
        "We focus on evaluating models' confidence in predictions before and after quantization in a zero-shot setting.\n",
        "In an ideal scenario, we expect the model's performance and confidence to remain consistent after quantization, preserving the initial calibration level.\n",
        "We evaluate the performance of LLMs post-compression using accuracy (Acc.) and calibration error (CE).\n",
        "\n",
        "In this notebook, we provide code for computing model confidence in answers, calibration errors and entropy.\n",
        "To run the notebook, you need to have predictions of models obtained with evaluation-harness framework.\n",
        "\n",
        "Running this code does not require GPU.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "I7zGaRzpEzhl"
      },
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "import json\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "from scipy import stats"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "1n4zQzOREv5U"
      },
      "outputs": [],
      "source": [
        "def ace(y_true: np.array, y_pred: np.array, num_ranges: int = 15) -> float:\n",
        "    \"\"\"\n",
        "     Measure the Adaptive Calibration Error (ACE) by [2], an version of the static calibration error that uses ranges\n",
        "     instead of bins. Every range contains the same number of predictions.\n",
        "\n",
        "    Parameters\n",
        "     ----------\n",
        "     y_true: np.array\n",
        "         True labels for each input.\n",
        "     y_pred: np.array\n",
        "         Categorical probability distribution for each input.\n",
        "     num_ranges: int\n",
        "         Number of ranges. Default is 10.\n",
        "\n",
        "     Returns\n",
        "     -------\n",
        "     float\n",
        "         Adaptive Calibration Error.\n",
        "    \"\"\"\n",
        "    assert len(y_pred.shape) == 2, \"y_pred must be a matrix!\"\n",
        "    assert (\n",
        "        y_true.shape[0] == y_pred.shape[0]\n",
        "    ), \"Shapes of y_true and y_pred do not match!\"\n",
        "\n",
        "    N = len(y_true)\n",
        "    num_classes = y_pred.shape[1]\n",
        "    confs = np.sort(np.max(y_pred, axis=1))\n",
        "    step = int(np.floor(N / num_ranges))  # Inputs per range\n",
        "    thresholds = np.repeat(\n",
        "        np.array([confs[i] for i in range(0, step * num_ranges, step)])[np.newaxis, ...], N, axis=0\n",
        "    )  # Get the thresholds corresponding to ranges\n",
        "\n",
        "    max_preds = np.repeat(\n",
        "        np.max(y_pred, axis=1)[..., np.newaxis], num_ranges, axis=1\n",
        "    )  # Repeat all maximum predictions\n",
        "    b = (max_preds <= thresholds).astype(\n",
        "        int\n",
        "    )  # Compare max predictions against thresholds\n",
        "    bin_indices = np.argmax(b, axis=1)\n",
        "    ace = 0\n",
        "\n",
        "    for bin in range(num_ranges):\n",
        "        bin_preds = y_pred[bin_indices == bin, :]\n",
        "        bin_labels = y_true[bin_indices == bin]\n",
        "\n",
        "        for k in range(num_classes):\n",
        "            bin_class_preds = bin_preds[bin_labels == k, :]\n",
        "\n",
        "            if bin_class_preds.shape[0] == 0:\n",
        "                continue\n",
        "\n",
        "            bin_class_acc = np.mean(\n",
        "                (np.argmax(bin_class_preds, axis=1) == k).astype(int)\n",
        "            )\n",
        "            bin_class_conf = np.mean(np.max(bin_class_preds, axis=1))\n",
        "            ace += abs(bin_class_acc - bin_class_conf)\n",
        "\n",
        "    ace /= num_classes * num_ranges\n",
        "\n",
        "    return ace\n",
        "def sce(y_true: np.array, y_pred: np.array, num_bins: int = 15) -> float:\n",
        "    \"\"\"\n",
        "    Measure the Static Calibration Error (SCE) by [2], an extension to the Expected Calibration Error to multiple\n",
        "    classes.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    y_true: np.array\n",
        "        True labels for each input.\n",
        "    y_pred: np.array\n",
        "        Categorical probability distribution for each input.\n",
        "    num_bins: int\n",
        "        Number of bins. Default is 10.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    float\n",
        "        Static Calibration Error.\n",
        "    \"\"\"\n",
        "    assert len(y_pred.shape) == 2, \"y_pred must be a matrix!\"\n",
        "    assert (\n",
        "        y_true.shape[0] == y_pred.shape[0]\n",
        "    ), \"Shapes of y_true and y_pred do not match!\"\n",
        "\n",
        "    N = len(y_true)\n",
        "    num_classes = y_pred.shape[1]\n",
        "    bins = np.arange(0, 1, 1 / num_bins)\n",
        "    bin_indices = np.digitize(np.max(y_pred, axis=1), bins)\n",
        "    sce = 0\n",
        "\n",
        "    for bin in range(num_bins):\n",
        "        bin_preds = y_pred[bin_indices == bin, :]\n",
        "        bin_labels = y_true[bin_indices == bin]\n",
        "\n",
        "        for k in range(num_classes):\n",
        "            bin_class_preds = bin_preds[bin_labels == k, :]\n",
        "\n",
        "            if bin_class_preds.shape[0] == 0:\n",
        "                continue\n",
        "\n",
        "            n_bk = bin_class_preds.shape[0]\n",
        "            bin_class_acc = np.mean(\n",
        "                (np.argmax(bin_class_preds, axis=1) == k).astype(float)\n",
        "            )\n",
        "            bin_class_conf = np.mean(np.max(bin_class_preds, axis=1))\n",
        "            sce += n_bk / N * abs(bin_class_acc - bin_class_conf)\n",
        "\n",
        "    sce /= num_classes\n",
        "\n",
        "    return sce\n",
        "\n",
        "def mce(y_true: np.array, y_pred: np.array, num_bins: int = 15) -> float:\n",
        "    \"\"\"\n",
        "    Measure the Maximum Calibration Error based on SCE metric\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    y_true: np.array\n",
        "        True labels for each input.\n",
        "    y_pred: np.array\n",
        "        Categorical probability distribution for each input.\n",
        "    num_bins: int\n",
        "        Number of bins. Default is 10.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    float\n",
        "        Static Calibration Error.\n",
        "    \"\"\"\n",
        "    assert len(y_pred.shape) == 2, \"y_pred must be a matrix!\"\n",
        "    assert (\n",
        "        y_true.shape[0] == y_pred.shape[0]\n",
        "    ), \"Shapes of y_true and y_pred do not match!\"\n",
        "\n",
        "    N = len(y_true)\n",
        "    num_classes = y_pred.shape[1]\n",
        "    bins = np.arange(0, 1, 1 / num_bins)\n",
        "    bin_indices = np.digitize(np.max(y_pred, axis=1), bins)\n",
        "    mce = -1\n",
        "\n",
        "    for bin in range(num_bins):\n",
        "        bin_preds = y_pred[bin_indices == bin, :]\n",
        "        bin_labels = y_true[bin_indices == bin]\n",
        "\n",
        "        for k in range(num_classes):\n",
        "            bin_class_preds = bin_preds[bin_labels == k, :]\n",
        "\n",
        "            if bin_class_preds.shape[0] == 0:\n",
        "                continue\n",
        "\n",
        "            n_bk = bin_class_preds.shape[0]\n",
        "            bin_class_acc = np.mean(\n",
        "                (np.argmax(bin_class_preds, axis=1) == k).astype(float)\n",
        "            )\n",
        "            bin_class_conf = np.mean(np.max(bin_class_preds, axis=1))\n",
        "            mce = max(mce, abs(bin_class_acc - bin_class_conf))\n",
        "\n",
        "    return mce\n",
        "\n",
        "\n",
        "def mce_binary(y_true: np.array, y_pred: np.array, num_bins: int = 100) -> float:\n",
        "    \"\"\"\n",
        "\n",
        "    Calculate the Expected Calibration Error: for each bin, the absolute difference between\n",
        "    the mean fraction of positives and the average predicted probability is taken. The ECE is\n",
        "    the weighed mean of these differences.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    y: np.ndarray\n",
        "        The true labels.\n",
        "    y_pred: np.ndarray\n",
        "        The predicted probabilities\n",
        "    num_bins: int\n",
        "        The number of bins to use.\n",
        "    Returns\n",
        "    -------\n",
        "    ece: float\n",
        "        The expected calibration error.\n",
        "    \"\"\"\n",
        "    n = len(y_pred)\n",
        "    bins = np.arange(0.0, 1.0, 1.0 / num_bins)\n",
        "    y_pred = np.max(y_pred, axis=-1)\n",
        "    bins_per_prediction = np.digitize(y_pred, bins)\n",
        "\n",
        "    df = pd.DataFrame({\"y_pred\": y_pred, \"y\": y_true, \"pred_bins\": bins_per_prediction})\n",
        "    # print(df)\n",
        "    grouped_by_bins = df.groupby(\"pred_bins\")\n",
        "    # calculate the mean y and predicted probabilities per bin\n",
        "    binned = grouped_by_bins.mean()\n",
        "\n",
        "    # calculate the number of items per bin\n",
        "    binned_counts = grouped_by_bins[\"y\"].count()\n",
        "\n",
        "    # calculate the proportion of data per bin\n",
        "    binned[\"weight\"] = binned_counts / n\n",
        "\n",
        "    weighed_diff = max(binned[\"y_pred\"] - binned[\"y\"])\n",
        "    return weighed_diff\n",
        "def ece(y_true: np.array, y_pred: np.array, n_bins: int = 100) -> float:\n",
        "    \"\"\"\n",
        "\n",
        "    Calculate the Expected Calibration Error: for each bin, the absolute difference between\n",
        "    the mean fraction of positives and the average predicted probability is taken. The ECE is\n",
        "    the weighed mean of these differences.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    y: np.ndarray\n",
        "        The true labels.\n",
        "    y_pred: np.ndarray\n",
        "        The predicted probabilities\n",
        "    n_bins: int\n",
        "        The number of bins to use.\n",
        "    Returns\n",
        "    -------\n",
        "    ece: float\n",
        "        The expected calibration error.\n",
        "    \"\"\"\n",
        "    n = len(y_pred)\n",
        "    bins = np.arange(0.0, 1.0, 1.0 / n_bins)\n",
        "    y_pred = np.max(y_pred, axis=-1)\n",
        "    bins_per_prediction = np.digitize(y_pred, bins)\n",
        "\n",
        "    df = pd.DataFrame({\"y_pred\": y_pred, \"y\": y_true, \"pred_bins\": bins_per_prediction})\n",
        "    # print(df)\n",
        "    grouped_by_bins = df.groupby(\"pred_bins\")\n",
        "    # calculate the mean y and predicted probabilities per bin\n",
        "    binned = grouped_by_bins.mean()\n",
        "\n",
        "    # calculate the number of items per bin\n",
        "    binned_counts = grouped_by_bins[\"y\"].count()\n",
        "\n",
        "    # calculate the proportion of data per bin\n",
        "    binned[\"weight\"] = binned_counts / n\n",
        "\n",
        "    weighed_diff = abs(binned[\"y_pred\"] - binned[\"y\"]) * binned[\"weight\"]\n",
        "    return weighed_diff.sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1_Quantization_AutoGPTQ.ipynb\t   5_awq_eval.ipynb\n",
            "2_LLMs_eval.ipynb\t\t   llama_c_result\n",
            "3_Calibration_Error_Metrics.ipynb  lm-evaluation-harness\n",
            "4_LLMs_eval_main_branch.ipynb\n"
          ]
        }
      ],
      "source": [
        "!ls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "_ALiWtdyB8Ni"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[PosixPath('lm-evaluation-harness/Jimmy1229'), PosixPath('lm-evaluation-harness/meta-llama')]\n"
          ]
        }
      ],
      "source": [
        "directory_path = Path('./lm-evaluation-harness/')\n",
        "all_models=[]\n",
        "for subdir in directory_path.iterdir():\n",
        "    if subdir.is_dir():\n",
        "        print_ = False\n",
        "        if \"Jimmy1229\" in str(subdir):\n",
        "            print_=True\n",
        "        if str(subdir).startswith(\"bigscience\"):\n",
        "            print_=True\n",
        "        if \"meta-llama\" in str(subdir):\n",
        "            print_=True\n",
        "        if print_:\n",
        "            all_models.append(subdir)\n",
        "print(all_models)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing models:   0%|          | 0/2 [00:00<?, ?it/s]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing models: 100%|██████████| 2/2 [00:00<00:00,  6.61it/s]\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'lm-evaluation-harness/Jimmy1229': {'piqa': 43.580000000000005,\n",
              "  'truthfulqa': 4.05},\n",
              " 'lm-evaluation-harness/meta-llama': {'piqa': 42.79,\n",
              "  'truthfulqa': 3.7600000000000002}}"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# dataset_names = ['boolq', 'hellaswag', 'piqa', 'arc', 'openbookqa', 'truthfulqa', 'xstory']\n",
        "dataset_names = ['piqa', 'truthfulqa']\n",
        "metrics = ['conf', 'conf_true', 'c_pos', 'c_neg', 'ace', 'mce', 'entropy']\n",
        "performance_metric_name = {\n",
        "    \"boolq\": ['acc'],\n",
        "    \"truthfulqa\": ['mc1', 'mc2'],\n",
        "    \"xstory\": ['acc']\n",
        "}\n",
        "combined_names = [f\"{dataset}_{metric}\" for dataset in dataset_names for metric in metrics]\n",
        "combined_names.append(\"model\")\n",
        "data_loaded_computed = {key: [] for key in combined_names}\n",
        "all_results=dict()\n",
        "\n",
        "for subdir in tqdm(all_models, desc='Processing models'):\n",
        "    path_subdir = next(subsubdir for subsubdir in subdir.iterdir() if subsubdir.is_dir())\n",
        "    data_loaded_computed['model'].append(str(subdir))\n",
        "    all_results[str(subdir)]={} \n",
        "    all_dict_ace={}\n",
        "    for _file in path_subdir.iterdir():\n",
        "        if _file.is_file() and 'write' in str(_file):\n",
        "            key_n = str(_file).split(\"/\")[-1].split()[-1]\n",
        "            # print(key_n)\n",
        "            dataset_name = key_n.split(\"_\")[0]\n",
        "            with open(_file, 'r') as file:\n",
        "                qa_data = json.load(file)\n",
        "            entropies_, conf_, conf_pos, conf_neg, conf_true, true_, probs, pred_ = [], [], [], [], [], [], [], []\n",
        "            shape_p = len([key for key in qa_data[0] if key.startswith('logit_')])\n",
        "            for data_i in qa_data:\n",
        "                true_label = data_i['truth']\n",
        "                # print(qa_data)\n",
        "                try:\n",
        "                    acc = data_i['acc']\n",
        "                except:\n",
        "                    acc = data_i['mc2']\n",
        "                logits = [data_i[key] for key in data_i if key.startswith('logit_')]\n",
        "                probabilities = np.exp(logits - np.max(logits)) / np.sum(np.exp(logits - np.max(logits)))\n",
        "                entropy = -np.sum(probabilities * np.log2(probabilities))\n",
        "                if probabilities.shape[0] == shape_p: # check if the probabilities are the same shape as the number of classes\n",
        "                    entropies_.append(entropy)\n",
        "                    try:    # for binary classification\n",
        "                        truth_ = 0 if \"yes\" in true_label else 1\n",
        "                    except: # for multi-class classification\n",
        "                        truth_=int(true_label)\n",
        "                    pred_i = probabilities[truth_] # get the probability of the true class\n",
        "                    conf_true.append(pred_i)\n",
        "                    true_.append(truth_)\n",
        "                    probs_i = probabilities.tolist()\n",
        "                    probs.append(probs_i)\n",
        "                    max_ = np.argmax(probabilities) # get the predicted class\n",
        "                    pred_.append(max_)\n",
        "                    conf_.append(probabilities[max_]) # get the probability of the predicted class\n",
        "                    (conf_pos if max_ == truth_ else conf_neg).append(probabilities[max_]) # get the probability of the predicted class if it is the true class\n",
        "            y_true = np.array(true_)\n",
        "            y_pred = np.array(probs)\n",
        "            metrics_data = {\n",
        "                'c_pos': np.mean(conf_pos),\n",
        "                'c_neg': np.mean(conf_neg),\n",
        "                'conf': np.mean(conf_),\n",
        "                'conf_true': np.mean(conf_true),\n",
        "                'ace': ace(y_true=y_true, y_pred=y_pred) if len(y_pred[0]) > 2 else ece(y_true=y_true, y_pred=y_pred),\n",
        "                'mce': mce(y_true=y_true, y_pred=y_pred, num_bins=100) if len(y_pred[0]) > 2 else mce_binary(y_true=y_true, y_pred=y_pred, num_bins=100),\n",
        "                'entropy': np.mean(entropies_)\n",
        "            }\n",
        "            for key, value in metrics_data.items():\n",
        "                metrics_data[key] = round(value, 4)\n",
        "                data_loaded_computed[f\"{dataset_name}_{key}\"].append(metrics_data[key])\n",
        "            all_dict_ace[dataset_name] = metrics_data['ace'] * 100\n",
        "            all_results[str(subdir)]=all_dict_ace\n",
        "all_results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "srzMPGOL8XfT"
      },
      "source": [
        "# Testing $H_0$ hypothesis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "qEXVZLrF5wtr"
      },
      "outputs": [],
      "source": [
        "with open('lm-evaluation-harness/meta-llama/Llama-3.2-1B/truthfulqa_mc_write_out_info.json', 'r') as file:\n",
        "    qa_data = json.load(file)\n",
        "with open('lm-evaluation-harness/Jimmy1229/Llama-3.2-1B-4bit/truthfulqa_mc_write_out_info.json', 'r') as file:\n",
        "    qa_data_8bit = json.load(file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "OdbudTa156fG"
      },
      "outputs": [],
      "source": [
        "pred_full=[]\n",
        "pred_quantized=[]\n",
        "for data in qa_data:\n",
        "    true_label=data['truth']\n",
        "    logit_keys = [key for key in data if key.startswith('logit_')]\n",
        "    logits = [data[key] for key in logit_keys]\n",
        "    probabilities = np.exp(logits - np.max(logits)) / np.sum(np.exp(logits - np.max(logits)))\n",
        "    # entropy = -np.sum(probabilities * np.log2(probabilities))\n",
        "    # entropies_.append(entropy)\n",
        "    try:\n",
        "        truth_ = 0 if \"yes\" in true_label else 1\n",
        "    except:\n",
        "        truth_=int(true_label)\n",
        "    pred_i = probabilities[truth_]\n",
        "    pred_full.append(pred_i)\n",
        "for data in qa_data_8bit:\n",
        "    true_label=data['truth']\n",
        "    logit_keys = [key for key in data if key.startswith('logit_')]\n",
        "    logits = [data[key] for key in logit_keys]\n",
        "    probabilities = np.exp(logits - np.max(logits)) / np.sum(np.exp(logits - np.max(logits)))\n",
        "    # entropy = -np.sum(probabilities * np.log2(probabilities))\n",
        "    # entropies_.append(entropy)\n",
        "    try:\n",
        "        truth_ = 0 if \"yes\" in true_label else 1\n",
        "    except:\n",
        "        truth_=int(true_label)\n",
        "    pred_i = probabilities[truth_]\n",
        "    pred_quantized.append(pred_i)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jdo_vV9rMHPm",
        "outputId": "d3f4c4fd-7cec-4e85-c904-3fc1d65c4713"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "There is no significant difference between the arrays.\n",
            "0.09434124985704687\n"
          ]
        }
      ],
      "source": [
        "# to compute stat.significance between 2 predictions, we use the t-test\n",
        "t_stat, p_value = stats.ttest_rel(pred_full, pred_quantized)\n",
        "alpha = 0.01\n",
        "if p_value < alpha:\n",
        "    print(\"There is a significant difference between the arrays.\")\n",
        "else:\n",
        "    print(\"There is no significant difference between the arrays.\")\n",
        "print(p_value)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
