{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0mT2KWpb6wWo"
      },
      "source": [
        "# Computing Calibration Metrics\n",
        "We focus on evaluating models' confidence in predictions before and after quantization in a zero-shot setting.\n",
        "In an ideal scenario, we expect the model's performance and confidence to remain consistent after quantization, preserving the initial calibration level.\n",
        "We evaluate the performance of LLMs post-compression using accuracy (Acc.) and calibration error (CE).\n",
        "\n",
        "In this notebook, we provide code for computing model confidence in answers, calibration errors and entropy.\n",
        "To run the notebook, you need to have predictions of models obtained with evaluation-harness framework.\n",
        "\n",
        "Running this code does not require GPU.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "I7zGaRzpEzhl"
      },
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "import json\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "from scipy import stats"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "1n4zQzOREv5U"
      },
      "outputs": [],
      "source": [
        "def ace(y_true: np.array, y_pred: np.array, num_ranges: int = 15) -> float:\n",
        "    \"\"\"\n",
        "     Measure the Adaptive Calibration Error (ACE) by [2], an version of the static calibration error that uses ranges\n",
        "     instead of bins. Every range contains the same number of predictions.\n",
        "\n",
        "    Parameters\n",
        "     ----------\n",
        "     y_true: np.array\n",
        "         True labels for each input.\n",
        "     y_pred: np.array\n",
        "         Categorical probability distribution for each input.\n",
        "     num_ranges: int\n",
        "         Number of ranges. Default is 10.\n",
        "\n",
        "     Returns\n",
        "     -------\n",
        "     float\n",
        "         Adaptive Calibration Error.\n",
        "    \"\"\"\n",
        "    assert len(y_pred.shape) == 2, \"y_pred must be a matrix!\"\n",
        "    assert (\n",
        "        y_true.shape[0] == y_pred.shape[0]\n",
        "    ), \"Shapes of y_true and y_pred do not match!\"\n",
        "\n",
        "    N = len(y_true)\n",
        "    num_classes = y_pred.shape[1]\n",
        "    confs = np.sort(np.max(y_pred, axis=1))\n",
        "    step = int(np.floor(N / num_ranges))  # Inputs per range\n",
        "    thresholds = np.repeat(\n",
        "        np.array([confs[i] for i in range(0, step * num_ranges, step)])[np.newaxis, ...], N, axis=0\n",
        "    )  # Get the thresholds corresponding to ranges\n",
        "\n",
        "    max_preds = np.repeat(\n",
        "        np.max(y_pred, axis=1)[..., np.newaxis], num_ranges, axis=1\n",
        "    )  # Repeat all maximum predictions\n",
        "    b = (max_preds <= thresholds).astype(\n",
        "        int\n",
        "    )  # Compare max predictions against thresholds\n",
        "    bin_indices = np.argmax(b, axis=1)\n",
        "    ace = 0\n",
        "\n",
        "    for bin in range(num_ranges):\n",
        "        bin_preds = y_pred[bin_indices == bin, :]\n",
        "        bin_labels = y_true[bin_indices == bin]\n",
        "\n",
        "        for k in range(num_classes):\n",
        "            bin_class_preds = bin_preds[bin_labels == k, :]\n",
        "\n",
        "            if bin_class_preds.shape[0] == 0:\n",
        "                continue\n",
        "\n",
        "            bin_class_acc = np.mean(\n",
        "                (np.argmax(bin_class_preds, axis=1) == k).astype(int)\n",
        "            )\n",
        "            bin_class_conf = np.mean(np.max(bin_class_preds, axis=1))\n",
        "            ace += abs(bin_class_acc - bin_class_conf)\n",
        "\n",
        "    ace /= num_classes * num_ranges\n",
        "\n",
        "    return ace\n",
        "def sce(y_true: np.array, y_pred: np.array, num_bins: int = 15) -> float:\n",
        "    \"\"\"\n",
        "    Measure the Static Calibration Error (SCE) by [2], an extension to the Expected Calibration Error to multiple\n",
        "    classes.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    y_true: np.array\n",
        "        True labels for each input.\n",
        "    y_pred: np.array\n",
        "        Categorical probability distribution for each input.\n",
        "    num_bins: int\n",
        "        Number of bins. Default is 10.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    float\n",
        "        Static Calibration Error.\n",
        "    \"\"\"\n",
        "    assert len(y_pred.shape) == 2, \"y_pred must be a matrix!\"\n",
        "    assert (\n",
        "        y_true.shape[0] == y_pred.shape[0]\n",
        "    ), \"Shapes of y_true and y_pred do not match!\"\n",
        "\n",
        "    N = len(y_true)\n",
        "    num_classes = y_pred.shape[1]\n",
        "    bins = np.arange(0, 1, 1 / num_bins)\n",
        "    bin_indices = np.digitize(np.max(y_pred, axis=1), bins)\n",
        "    sce = 0\n",
        "\n",
        "    for bin in range(num_bins):\n",
        "        bin_preds = y_pred[bin_indices == bin, :]\n",
        "        bin_labels = y_true[bin_indices == bin]\n",
        "\n",
        "        for k in range(num_classes):\n",
        "            bin_class_preds = bin_preds[bin_labels == k, :]\n",
        "\n",
        "            if bin_class_preds.shape[0] == 0:\n",
        "                continue\n",
        "\n",
        "            n_bk = bin_class_preds.shape[0]\n",
        "            bin_class_acc = np.mean(\n",
        "                (np.argmax(bin_class_preds, axis=1) == k).astype(float)\n",
        "            )\n",
        "            bin_class_conf = np.mean(np.max(bin_class_preds, axis=1))\n",
        "            sce += n_bk / N * abs(bin_class_acc - bin_class_conf)\n",
        "\n",
        "    sce /= num_classes\n",
        "\n",
        "    return sce\n",
        "\n",
        "def mce(y_true: np.array, y_pred: np.array, num_bins: int = 15) -> float:\n",
        "    \"\"\"\n",
        "    Measure the Maximum Calibration Error based on SCE metric\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    y_true: np.array\n",
        "        True labels for each input.\n",
        "    y_pred: np.array\n",
        "        Categorical probability distribution for each input.\n",
        "    num_bins: int\n",
        "        Number of bins. Default is 10.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    float\n",
        "        Static Calibration Error.\n",
        "    \"\"\"\n",
        "    assert len(y_pred.shape) == 2, \"y_pred must be a matrix!\"\n",
        "    assert (\n",
        "        y_true.shape[0] == y_pred.shape[0]\n",
        "    ), \"Shapes of y_true and y_pred do not match!\"\n",
        "\n",
        "    N = len(y_true)\n",
        "    num_classes = y_pred.shape[1]\n",
        "    bins = np.arange(0, 1, 1 / num_bins)\n",
        "    bin_indices = np.digitize(np.max(y_pred, axis=1), bins)\n",
        "    mce = -1\n",
        "\n",
        "    for bin in range(num_bins):\n",
        "        bin_preds = y_pred[bin_indices == bin, :]\n",
        "        bin_labels = y_true[bin_indices == bin]\n",
        "\n",
        "        for k in range(num_classes):\n",
        "            bin_class_preds = bin_preds[bin_labels == k, :]\n",
        "\n",
        "            if bin_class_preds.shape[0] == 0:\n",
        "                continue\n",
        "\n",
        "            n_bk = bin_class_preds.shape[0]\n",
        "            bin_class_acc = np.mean(\n",
        "                (np.argmax(bin_class_preds, axis=1) == k).astype(float)\n",
        "            )\n",
        "            bin_class_conf = np.mean(np.max(bin_class_preds, axis=1))\n",
        "            mce = max(mce, abs(bin_class_acc - bin_class_conf))\n",
        "\n",
        "    return mce\n",
        "\n",
        "\n",
        "def mce_binary(y_true: np.array, y_pred: np.array, num_bins: int = 100) -> float:\n",
        "    \"\"\"\n",
        "\n",
        "    Calculate the Expected Calibration Error: for each bin, the absolute difference between\n",
        "    the mean fraction of positives and the average predicted probability is taken. The ECE is\n",
        "    the weighed mean of these differences.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    y: np.ndarray\n",
        "        The true labels.\n",
        "    y_pred: np.ndarray\n",
        "        The predicted probabilities\n",
        "    num_bins: int\n",
        "        The number of bins to use.\n",
        "    Returns\n",
        "    -------\n",
        "    ece: float\n",
        "        The expected calibration error.\n",
        "    \"\"\"\n",
        "    n = len(y_pred)\n",
        "    bins = np.arange(0.0, 1.0, 1.0 / num_bins)\n",
        "    y_pred = np.max(y_pred, axis=-1)\n",
        "    bins_per_prediction = np.digitize(y_pred, bins)\n",
        "\n",
        "    df = pd.DataFrame({\"y_pred\": y_pred, \"y\": y_true, \"pred_bins\": bins_per_prediction})\n",
        "    # print(df)\n",
        "    grouped_by_bins = df.groupby(\"pred_bins\")\n",
        "    # calculate the mean y and predicted probabilities per bin\n",
        "    binned = grouped_by_bins.mean()\n",
        "\n",
        "    # calculate the number of items per bin\n",
        "    binned_counts = grouped_by_bins[\"y\"].count()\n",
        "\n",
        "    # calculate the proportion of data per bin\n",
        "    binned[\"weight\"] = binned_counts / n\n",
        "\n",
        "    weighed_diff = max(binned[\"y_pred\"] - binned[\"y\"])\n",
        "    return weighed_diff\n",
        "def ece(y_true: np.array, y_pred: np.array, n_bins: int = 100) -> float:\n",
        "    \"\"\"\n",
        "\n",
        "    Calculate the Expected Calibration Error: for each bin, the absolute difference between\n",
        "    the mean fraction of positives and the average predicted probability is taken. The ECE is\n",
        "    the weighed mean of these differences.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    y: np.ndarray\n",
        "        The true labels.\n",
        "    y_pred: np.ndarray\n",
        "        The predicted probabilities\n",
        "    n_bins: int\n",
        "        The number of bins to use.\n",
        "    Returns\n",
        "    -------\n",
        "    ece: float\n",
        "        The expected calibration error.\n",
        "    \"\"\"\n",
        "    n = len(y_pred)\n",
        "    bins = np.arange(0.0, 1.0, 1.0 / n_bins)\n",
        "    y_pred = np.max(y_pred, axis=-1)\n",
        "    bins_per_prediction = np.digitize(y_pred, bins)\n",
        "\n",
        "    df = pd.DataFrame({\"y_pred\": y_pred, \"y\": y_true, \"pred_bins\": bins_per_prediction})\n",
        "    # print(df)\n",
        "    grouped_by_bins = df.groupby(\"pred_bins\")\n",
        "    # calculate the mean y and predicted probabilities per bin\n",
        "    binned = grouped_by_bins.mean()\n",
        "\n",
        "    # calculate the number of items per bin\n",
        "    binned_counts = grouped_by_bins[\"y\"].count()\n",
        "\n",
        "    # calculate the proportion of data per bin\n",
        "    binned[\"weight\"] = binned_counts / n\n",
        "\n",
        "    weighed_diff = abs(binned[\"y_pred\"] - binned[\"y\"]) * binned[\"weight\"]\n",
        "    return weighed_diff.sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1_Quantization_AutoGPTQ.ipynb  3_Calibration_Error_Metrics.ipynb\n",
            "2_LLMs_eval.ipynb\t       lm-evaluation-harness\n"
          ]
        }
      ],
      "source": [
        "!ls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "_ALiWtdyB8Ni"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "lm-evaluation-harness/.git\n",
            "lm-evaluation-harness/bigscience\n",
            "lm-evaluation-harness/docs\n",
            "lm-evaluation-harness/iproskurina\n",
            "lm-evaluation-harness/lm_eval\n",
            "lm-evaluation-harness/lm_eval.egg-info\n",
            "lm-evaluation-harness/results\n",
            "lm-evaluation-harness/scripts\n",
            "lm-evaluation-harness/templates\n",
            "lm-evaluation-harness/tests\n",
            "[PosixPath('lm-evaluation-harness/bigscience'), PosixPath('lm-evaluation-harness/iproskurina')]\n"
          ]
        }
      ],
      "source": [
        "directory_path = Path('./lm-evaluation-harness/')\n",
        "all_models=[]\n",
        "for subdir in directory_path.iterdir():\n",
        "    if subdir.is_dir():\n",
        "        print(subdir)\n",
        "        print_ = False\n",
        "        if \"iproskurina\" in str(subdir):\n",
        "            print_=True\n",
        "        if str(subdir).startswith(\"bigscience\"):\n",
        "            print_=True\n",
        "        if \"bigscience\" in str(subdir):\n",
        "            print_=True\n",
        "        if print_:\n",
        "            all_models.append(subdir)\n",
        "print(all_models)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing models:   0%|          | 0/2 [00:00<?, ?it/s]\n"
          ]
        },
        {
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'lm-evaluation-harness/lm-evaluation-harness/bigscience'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[58], line 12\u001b[0m\n\u001b[1;32m      9\u001b[0m all_results[\u001b[38;5;28mstr\u001b[39m(subdir)]\u001b[38;5;241m=\u001b[39m{} \n\u001b[1;32m     10\u001b[0m all_dict_ace\u001b[38;5;241m=\u001b[39m{}\n\u001b[0;32m---> 12\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m path_subdir\u001b[38;5;241m.\u001b[39miterdir():\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;28mprint\u001b[39m(path_subdir\u001b[38;5;241m.\u001b[39miterdir())\n",
            "File \u001b[0;32m/usr/lib/python3.10/pathlib.py:1017\u001b[0m, in \u001b[0;36mPath.iterdir\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1013\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21miterdir\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m   1014\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Iterate over the files in this directory.  Does not yield any\u001b[39;00m\n\u001b[1;32m   1015\u001b[0m \u001b[38;5;124;03m    result for the special paths '.' and '..'.\u001b[39;00m\n\u001b[1;32m   1016\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1017\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_accessor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlistdir\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m:\n\u001b[1;32m   1018\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m..\u001b[39m\u001b[38;5;124m'\u001b[39m}:\n\u001b[1;32m   1019\u001b[0m             \u001b[38;5;66;03m# Yielding a path object for these makes little sense\u001b[39;00m\n\u001b[1;32m   1020\u001b[0m             \u001b[38;5;28;01mcontinue\u001b[39;00m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'lm-evaluation-harness/lm-evaluation-harness/bigscience'"
          ]
        }
      ],
      "source": [
        "combined_names = [f\"{dataset}_{metric}\" for dataset in dataset_names for metric in metrics]\n",
        "combined_names.append(\"model\")\n",
        "data_loaded_computed = {key: [] for key in combined_names}\n",
        "all_results=dict()\n",
        "# print(all_models) [PosixPath('lm-evaluation-harness/bigscience'), PosixPath('lm-evaluation-harness/iproskurina')]\n",
        "for subdir in tqdm(all_models, desc='Processing models'):\n",
        "    path_subdir = directory_path / subdir #lm-evaluation-harness/lm-evaluation-harness/bigscience\n",
        "    data_loaded_computed['model'].append(str(subdir))\n",
        "    all_results[str(subdir)]={} \n",
        "    all_dict_ace={}\n",
        "    \n",
        "    for file in path_subdir.iterdir():\n",
        "        print(path_subdir.iterdir())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ecUrCb091cKO",
        "outputId": "e10e4668-e77f-4a73-8517-a654533760a3"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing models: 100%|██████████| 2/2 [00:00<00:00, 4707.41it/s]\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{}"
            ]
          },
          "execution_count": 51,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# dataset_names = ['hellaswag', 'piqa', 'arc', 'openbookqa', 'truthfulqa', 'xstory']\n",
        "dataset_names = ['hellaswag']\n",
        "metrics = ['conf', 'conf_true', 'c_pos', 'c_neg', 'ace', 'mce', 'entropy']\n",
        "performance_metric_name = {\n",
        "    \"boolq\": ['acc'],\n",
        "    \"truthfulqa\": ['mc1', 'mc2'],\n",
        "    \"xstory\": ['acc']\n",
        "}\n",
        "combined_names = [f\"{dataset}_{metric}\" for dataset in dataset_names for metric in metrics]\n",
        "combined_names.append(\"model\")\n",
        "data_loaded_computed = {key: [] for key in combined_names}\n",
        "all_results=dict()\n",
        "for subdir in tqdm(all_models, desc='Processing models'):\n",
        "    if 'opt' in str(subdir) or '7b' not in str(subdir):\n",
        "        continue\n",
        "    path_subdir = directory_path / subdir\n",
        "    data_loaded_computed['model'].append(str(subdir))\n",
        "    p_filename = f\"{subdir}.json\" # wrong\n",
        "    performance_file = directory_path / p_filename\n",
        "    all_results[str(subdir)]={} \n",
        "    all_dict_ace={}\n",
        "    if performance_file.exists():\n",
        "        with open(performance_file, 'r') as file:\n",
        "            d_perf = json.load(file)\n",
        "\n",
        "    for _file in path_subdir.iterdir():\n",
        "        if _file.is_file() and 'write' in str(_file):\n",
        "            key_n = str(_file).split(\"/\")[-1].split()[-1]\n",
        "            dataset_name = key_n.split(\"_\")[0]\n",
        "\n",
        "            with open(_file, 'r') as file:\n",
        "                qa_data = json.load(file)\n",
        "\n",
        "            entropies_, conf_, conf_pos, conf_neg, conf_true, true_, probs, pred_ = [], [], [], [], [], [], [], []\n",
        "            shape_p = len([key for key in qa_data[0] if key.startswith('logit_')])\n",
        "\n",
        "            for data_i in qa_data:\n",
        "                true_label = data_i['truth']\n",
        "                logits = [data_i[key] for key in data_i if key.startswith('logit_')]\n",
        "                probabilities = np.exp(logits - np.max(logits)) / np.sum(np.exp(logits - np.max(logits)))\n",
        "                entropy = -np.sum(probabilities * np.log2(probabilities))\n",
        "                if probabilities.shape[0] == shape_p:\n",
        "                    entropies_.append(entropy)\n",
        "                    try:\n",
        "                        truth_ = 0 if \"yes\" in true_label else 1\n",
        "                    except:\n",
        "                        truth_=int(true_label)\n",
        "                    pred_i = probabilities[truth_]\n",
        "                    conf_true.append(pred_i)\n",
        "                    true_.append(truth_)\n",
        "                    probs_i = probabilities.tolist()\n",
        "                    probs.append(probs_i)\n",
        "                    max_ = np.argmax(probabilities)\n",
        "                    pred_.append(max_)\n",
        "                    conf_.append(probabilities[max_])\n",
        "                    (conf_pos if max_ == truth_ else conf_neg).append(probabilities[max_])\n",
        "            y_true=np.array(true_)\n",
        "            y_pred=np.array(probs)\n",
        "            metrics_data = {\n",
        "                'c_pos': np.mean(conf_pos),\n",
        "                'c_neg': np.mean(conf_neg),\n",
        "                'conf': np.mean(conf_),\n",
        "                'conf_true': np.mean(conf_true),\n",
        "                'ace': ace(y_true=y_true, y_pred=y_pred) if len(y_pred[0]) > 2 else ece(y_true=y_true, y_pred=y_pred),\n",
        "                'mce': mce(y_true=y_true, y_pred=y_pred, num_bins=100) if len(y_pred[0]) > 2 else mce_binary(y_true=y_true, y_pred=y_pred, num_bins=100),\n",
        "                'entropy': np.mean(entropies_)\n",
        "            }\n",
        "\n",
        "            for key, value in metrics_data.items():\n",
        "                metrics_data[key] = round(value, 4)\n",
        "                data_loaded_computed[f\"{dataset_name}_{key}\"].append(metrics_data[key])\n",
        "\n",
        "            all_dict_ace[dataset_name] = metrics_data['ace'] * 100\n",
        "            all_results[str(subdir)]=all_dict_ace\n",
        "all_results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "srzMPGOL8XfT"
      },
      "source": [
        "# Testing $H_0$ hypothesis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "qEXVZLrF5wtr"
      },
      "outputs": [
        {
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: './bigscience-bloom-7b1/hellaswag_write_out_info.json'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[23], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m./bigscience-bloom-7b1/hellaswag_write_out_info.json\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m file:\n\u001b[1;32m      2\u001b[0m     qa_data \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mload(file)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./iproskurina-bloom-7b1-gptq-4bit/hellaswag_write_out_info.json\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m file:\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/IPython/core/interactiveshell.py:324\u001b[0m, in \u001b[0;36m_modified_open\u001b[0;34m(file, *args, **kwargs)\u001b[0m\n\u001b[1;32m    317\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[1;32m    318\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    319\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    320\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    321\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    322\u001b[0m     )\n\u001b[0;32m--> 324\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './bigscience-bloom-7b1/hellaswag_write_out_info.json'"
          ]
        }
      ],
      "source": [
        "with open('./bigscience-bloom-7b1/hellaswag_write_out_info.json', 'r') as file:\n",
        "    qa_data = json.load(file)\n",
        "with open('./iproskurina-bloom-7b1-gptq-4bit/hellaswag_write_out_info.json', 'r') as file:\n",
        "    qa_data_8bit = json.load(file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {
        "id": "OdbudTa156fG"
      },
      "outputs": [],
      "source": [
        "pred_full=[]\n",
        "pred_quantized=[]\n",
        "for data in qa_data:\n",
        "    true_label=data['truth']\n",
        "    logit_keys = [key for key in data if key.startswith('logit_')]\n",
        "    logits = [data[key] for key in logit_keys]\n",
        "    probabilities = np.exp(logits - np.max(logits)) / np.sum(np.exp(logits - np.max(logits)))\n",
        "    # entropy = -np.sum(probabilities * np.log2(probabilities))\n",
        "    # entropies_.append(entropy)\n",
        "    try:\n",
        "        truth_ = 0 if \"yes\" in true_label else 1\n",
        "    except:\n",
        "        truth_=int(true_label)\n",
        "    pred_i = probabilities[truth_]\n",
        "    pred_full.append(pred_i)\n",
        "for data in qa_data_8bit:\n",
        "    true_label=data['truth']\n",
        "    logit_keys = [key for key in data if key.startswith('logit_')]\n",
        "    logits = [data[key] for key in logit_keys]\n",
        "    probabilities = np.exp(logits - np.max(logits)) / np.sum(np.exp(logits - np.max(logits)))\n",
        "    # entropy = -np.sum(probabilities * np.log2(probabilities))\n",
        "    # entropies_.append(entropy)\n",
        "    try:\n",
        "        truth_ = 0 if \"yes\" in true_label else 1\n",
        "    except:\n",
        "        truth_=int(true_label)\n",
        "    pred_i = probabilities[truth_]\n",
        "    pred_quantized.append(pred_i)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jdo_vV9rMHPm",
        "outputId": "d3f4c4fd-7cec-4e85-c904-3fc1d65c4713"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "There is a significant difference between the arrays.\n",
            "4.1747311644578645e-28\n"
          ]
        }
      ],
      "source": [
        "# to compute stat.significance between 2 predictions, we use the t-test\n",
        "t_stat, p_value = stats.ttest_rel(pred_full, pred_quantized)\n",
        "alpha = 0.01\n",
        "if p_value < alpha:\n",
        "    print(\"There is a significant difference between the arrays.\")\n",
        "else:\n",
        "    print(\"There is no significant difference between the arrays.\")\n",
        "print(p_value)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
