{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: fineGrained).\n",
      "The token `3B` has been saved to /root/.cache/huggingface/stored_tokens\n",
      "Your token has been saved to /root/.cache/huggingface/token\n",
      "Login successful.\n",
      "The current active token is: `3B`\n"
     ]
    }
   ],
   "source": [
    "!huggingface-cli login --token "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/root/quantized-lm-confidence/notebooks/lm-evaluation-harness\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/.local/lib/python3.10/site-packages/IPython/core/magics/osm.py:417: UserWarning: This is now an optional IPython functionality, setting dhist requires you to install the `pickleshare` library.\n",
      "  self.shell.db['dhist'] = compress_dhist(dhist)[-100:]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Already on 'main'\n",
      "Your branch is up to date with 'origin/main'.\n"
     ]
    }
   ],
   "source": [
    "%cd lm-evaluation-harness\n",
    "!git checkout \"main\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Go to https://github.com/EleutherAI/lm-evaluation-harness/issues/1254"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = \"Llama-3.2-1B\"#@param {type:\"string\"}\n",
    "task  = \"hellaswag\"#@param {type:\"string\"}\n",
    "model_path=\"MaziyarPanahi/Llama-3.2-1B-GGUF\"#@param {type:\"string\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MaziyarPanahi/Llama-3.2-1B-GGUF\n"
     ]
    }
   ],
   "source": [
    "output_path = model_path\n",
    "print(output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 -m llama_cpp.server --model models/7B/llama-model.gguf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-01-12:20:11:13,878 INFO     [__main__.py:279] Verbosity set to DEBUG\n",
      "2025-01-12:20:11:39,359 INFO     [__main__.py:376] Selected Tasks: ['hellaswag']\n",
      "2025-01-12:20:11:39,373 INFO     [evaluator.py:164] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234 | Setting fewshot manual seed to 1234\n",
      "2025-01-12:20:11:39,373 INFO     [evaluator.py:201] Initializing hf model, with arguments: {'pretrained': 'MaziyarPanahi/Llama-3.2-1B-GGUF'}\n",
      "2025-01-12:20:11:39,459 INFO     [huggingface.py:132] Using device 'cuda:0'\n",
      "2025-01-12:20:11:40,099 DEBUG    [huggingface.py:485] Using model type 'causal'\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/bin/lm_eval\", line 8, in <module>\n",
      "    sys.exit(cli_evaluate())\n",
      "  File \"/root/quantized-lm-confidence/notebooks/lm-evaluation-harness/lm_eval/__main__.py\", line 382, in cli_evaluate\n",
      "    results = evaluator.simple_evaluate(\n",
      "  File \"/root/quantized-lm-confidence/notebooks/lm-evaluation-harness/lm_eval/utils.py\", line 402, in _wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/root/quantized-lm-confidence/notebooks/lm-evaluation-harness/lm_eval/evaluator.py\", line 204, in simple_evaluate\n",
      "    lm = lm_eval.api.registry.get_model(model).create_from_arg_string(\n",
      "  File \"/root/quantized-lm-confidence/notebooks/lm-evaluation-harness/lm_eval/api/model.py\", line 147, in create_from_arg_string\n",
      "    return cls(**args, **args2)\n",
      "  File \"/root/quantized-lm-confidence/notebooks/lm-evaluation-harness/lm_eval/models/huggingface.py\", line 175, in __init__\n",
      "    self._create_tokenizer(\n",
      "  File \"/root/quantized-lm-confidence/notebooks/lm-evaluation-harness/lm_eval/models/huggingface.py\", line 707, in _create_tokenizer\n",
      "    self.tokenizer = transformers.AutoTokenizer.from_pretrained(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/transformers/models/auto/tokenization_auto.py\", line 940, in from_pretrained\n",
      "    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py\", line 2016, in from_pretrained\n",
      "    raise EnvironmentError(\n",
      "OSError: Can't load tokenizer for 'MaziyarPanahi/Llama-3.2-1B-GGUF'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'MaziyarPanahi/Llama-3.2-1B-GGUF' is the correct path to a directory containing all relevant files for a LlamaTokenizerFast tokenizer.\n"
     ]
    }
   ],
   "source": [
    "!lm_eval --model hf \\\n",
    "    --model_args pretrained=$model_path\\\n",
    "    --tasks $task \\\n",
    "    --device cuda:0 \\\n",
    "    --write_out \\\n",
    "    --output_path $output_path \\\n",
    "    --log_samples \\\n",
    "    --verbosity DEBUG\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "usage: lm_eval [-h] [--model MODEL] [--tasks task1,task2]\n",
      "               [--model_args MODEL_ARGS] [--num_fewshot N]\n",
      "               [--batch_size auto|auto:N|N] [--max_batch_size N]\n",
      "               [--device DEVICE] [--output_path DIR|DIR/file.json]\n",
      "               [--limit N|0<N<1] [--use_cache DIR]\n",
      "               [--cache_requests {true,refresh,delete}] [--check_integrity]\n",
      "               [--write_out] [--log_samples]\n",
      "               [--system_instruction SYSTEM_INSTRUCTION]\n",
      "               [--apply_chat_template [APPLY_CHAT_TEMPLATE]]\n",
      "               [--fewshot_as_multiturn] [--show_config] [--include_path DIR]\n",
      "               [--gen_kwargs GEN_KWARGS]\n",
      "               [--verbosity CRITICAL|ERROR|WARNING|INFO|DEBUG]\n",
      "               [--wandb_args WANDB_ARGS] [--hf_hub_log_args HF_HUB_LOG_ARGS]\n",
      "               [--predict_only] [--seed SEED] [--trust_remote_code]\n",
      "\n",
      "options:\n",
      "  -h, --help            show this help message and exit\n",
      "  --model MODEL, -m MODEL\n",
      "                        Name of model e.g. `hf`\n",
      "  --tasks task1,task2, -t task1,task2\n",
      "                        Comma-separated list of task names or task groupings to evaluate on.\n",
      "                        To get full list of tasks, use one of the commands `lm-eval --tasks {{list_groups,list_subtasks,list_tags,list}}` to list out all available names for task groupings; only (sub)tasks; tags; or all of the above\n",
      "  --model_args MODEL_ARGS, -a MODEL_ARGS\n",
      "                        Comma separated string arguments for model, e.g. `pretrained=EleutherAI/pythia-160m,dtype=float32`\n",
      "  --num_fewshot N, -f N\n",
      "                        Number of examples in few-shot context\n",
      "  --batch_size auto|auto:N|N, -b auto|auto:N|N\n",
      "                        Acceptable values are 'auto', 'auto:N' or N, where N is an integer. Default 1.\n",
      "  --max_batch_size N    Maximal batch size to try with --batch_size auto.\n",
      "  --device DEVICE       Device to use (e.g. cuda, cuda:0, cpu).\n",
      "  --output_path DIR|DIR/file.json, -o DIR|DIR/file.json\n",
      "                        The path to the output file where the result metrics will be saved. If the path is a directory and log_samples is true, the results will be saved in the directory. Else the parent directory will be used.\n",
      "  --limit N|0<N<1, -L N|0<N<1\n",
      "                        Limit the number of examples per task. If <1, limit is a percentage of the total number of examples.\n",
      "  --use_cache DIR, -c DIR\n",
      "                        A path to a sqlite db file for caching model responses. `None` if not caching.\n",
      "  --cache_requests {true,refresh,delete}\n",
      "                        Speed up evaluation by caching the building of dataset requests. `None` if not caching.\n",
      "  --check_integrity     Whether to run the relevant part of the test suite for the tasks.\n",
      "  --write_out, -w       Prints the prompt for the first few documents.\n",
      "  --log_samples, -s     If True, write out all model outputs and documents for per-sample measurement and post-hoc analysis. Use with --output_path.\n",
      "  --system_instruction SYSTEM_INSTRUCTION\n",
      "                        System instruction to be used in the prompt\n",
      "  --apply_chat_template [APPLY_CHAT_TEMPLATE]\n",
      "                        If True, apply chat template to the prompt. Providing `--apply_chat_template` without an argument will apply the default chat template to the prompt. To apply a specific template from the available list of templates, provide the template name as an argument. E.g. `--apply_chat_template template_name`\n",
      "  --fewshot_as_multiturn\n",
      "                        If True, uses the fewshot as a multi-turn conversation\n",
      "  --show_config         If True, shows the the full config of all tasks at the end of the evaluation.\n",
      "  --include_path DIR    Additional path to include if there are external tasks to include.\n",
      "  --gen_kwargs GEN_KWARGS\n",
      "                        String arguments for model generation on greedy_until tasks, e.g. `temperature=0,top_k=0,top_p=0`.\n",
      "  --verbosity CRITICAL|ERROR|WARNING|INFO|DEBUG, -v CRITICAL|ERROR|WARNING|INFO|DEBUG\n",
      "                        Controls the reported logging error level. Set to DEBUG when testing + adding new task configurations for comprehensive log output.\n",
      "  --wandb_args WANDB_ARGS\n",
      "                        Comma separated string arguments passed to wandb.init, e.g. `project=lm-eval,job_type=eval\n",
      "  --hf_hub_log_args HF_HUB_LOG_ARGS\n",
      "                        Comma separated string arguments passed to Hugging Face Hub's log function, e.g. `hub_results_org=EleutherAI,hub_repo_name=lm-eval-results`\n",
      "  --predict_only, -x    Use with --log_samples. Only model outputs will be saved and metrics will not be evaluated.\n",
      "  --seed SEED           Set seed for python's random, numpy, torch, and fewshot sampling.\n",
      "                        Accepts a comma-separated list of 4 values for python's random, numpy, torch, and fewshot sampling seeds, respectively, or a single integer to set the same seed for all four.\n",
      "                        The values are either an integer or 'None' to not set the seed. Default is `0,1234,1234,1234` (for backward compatibility).\n",
      "                        E.g. `--seed 0,None,8,52` sets `random.seed(0)`, `torch.manual_seed(8)`, and fewshot sampling seed to 52. Here numpy's seed is not set since the second value is `None`.\n",
      "                        E.g, `--seed 42` sets all four seeds to 42.\n",
      "  --trust_remote_code   Sets trust_remote_code to True to execute code to create HF Datasets from the Hub\n"
     ]
    }
   ],
   "source": [
    "!lm_eval -h"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
