{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!nvcc --version"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4dFKsPaea30b",
        "outputId": "38b8ebc2-6eae-4547-f7f7-9ab478f8e836"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nvcc: NVIDIA (R) Cuda compiler driver\n",
            "Copyright (c) 2005-2022 NVIDIA Corporation\n",
            "Built on Wed_Sep_21_10:33:58_PDT_2022\n",
            "Cuda compilation tools, release 11.8, V11.8.89\n",
            "Build cuda_11.8.r11.8/compiler.31833905_0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install auto-gptq --extra-index-url https://huggingface.github.io/autogptq-index/whl/cu118/\n",
        "# !pip install transformers==4.39"
      ],
      "metadata": {
        "id": "bB0eBbo0bDea"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Auto-GPTQ: https://pypi.org/project/auto-gptq/0.2.0/"
      ],
      "metadata": {
        "id": "WRu9bhYphdD4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, TextGenerationPipeline\n",
        "from auto_gptq import AutoGPTQForCausalLM, BaseQuantizeConfig\n",
        "import logging\n",
        "\n",
        "logging.basicConfig(\n",
        "    format=\"%(asctime)s %(levelname)s [%(name)s] %(message)s\", level=logging.INFO, datefmt=\"%Y-%m-%d %H:%M:%S\"\n",
        ")\n",
        "\n",
        "pretrained_model_dir = \"facebook/opt-125m\"\n",
        "quantized_model_dir = \"opt-125m-4bit\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(pretrained_model_dir, use_fast=True)\n",
        "examples = [\n",
        "    tokenizer(\n",
        "        \"auto-gptq is an easy-to-use model quantization library with user-friendly apis, based on GPTQ algorithm.\"\n",
        "    )\n",
        "]\n",
        "\n",
        "quantize_config = BaseQuantizeConfig(\n",
        "    bits=4,  # quantize model to 4-bit\n",
        "    group_size=128,  # it is recommended to set the value to 128\n",
        "    desc_act=False,  # set to False can significantly speed up inference but the perplexity may slightly bad\n",
        ")\n",
        "\n",
        "# load un-quantized model, by default, the model will always be loaded into CPU memory\n",
        "model = AutoGPTQForCausalLM.from_pretrained(pretrained_model_dir, quantize_config)\n",
        "\n",
        "# quantize model, the examples should be list of dict whose keys can only be \"input_ids\" and \"attention_mask\"\n",
        "model.quantize(examples)\n",
        "model.save_quantized(quantized_model_dir, use_safetensors=True)\n",
        "\n",
        "# load quantized model to the first GPU\n",
        "model = AutoGPTQForCausalLM.from_quantized(quantized_model_dir, device=\"cuda:0\")\n",
        "\n",
        "# inference with model.generate\n",
        "print(tokenizer.decode(model.generate(**tokenizer(\"auto_gptq is\", return_tensors=\"pt\").to(model.device))[0]))\n",
        "\n",
        "# or you can also use pipeline\n",
        "pipeline = TextGenerationPipeline(model=model, tokenizer=tokenizer)\n",
        "print(pipeline(\"auto-gptq is\")[0][\"generated_text\"])"
      ],
      "metadata": {
        "id": "DucYBSidbI0X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# push quantized model to Hugging Face Hub.\n",
        "# to use use_auth_token=True, Login first via huggingface-cli login.\n",
        "# or pass explcit token with: use_auth_token=\"hf_xxxxxxx\"\n",
        "# (uncomment the following three lines to enable this feature)\n",
        "# repo_id = f\"YourUserName/{quantized_model_dir}\"\n",
        "# commit_message = f\"AutoGPTQ model for {pretrained_model_dir}: {quantize_config.bits}bits, gr{quantize_config.group_size}, desc_act={quantize_config.desc_act}\"\n",
        "# model.push_to_hub(repo_id, commit_message=commit_message, use_auth_token=True)\n",
        "# tokenizer.push_to_hub(repo_id, commit_message=commit_message, use_auth_token=True)"
      ],
      "metadata": {
        "id": "g9gLYc49b2fr"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}